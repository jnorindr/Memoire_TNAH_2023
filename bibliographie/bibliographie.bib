@misc{albouyIiifdownloader2023,
  title = {Iiif-Downloader},
  author = {Albouy, S{\'e}gol{\`e}ne},
  year = {2023},
  month = jun,
  url = {https://github.com/Segolene-Albouy/iiif-downloader},
  urldate = {2023-08-23},
  abstract = {Script for automatic image retrieval from IIIF manifests},
  copyright = {MIT}
}

@misc{APIRESTEst,
  title = {{Une API REST, c'est quoi ?}},
  url = {https://www.redhat.com/fr/topics/api/what-is-a-rest-api},
  urldate = {2023-08-27},
  abstract = {Une API REST est une interface de programmation d'application qui respecte les contraintes de l'architecture REST},
  langid = {french},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/2F44WKB2/what-is-a-rest-api.html}
}

@misc{APIRMNGrandPalais,
  title = {{{API}} de La {{RMN-Grand Palais}}},
  url = {https://api.art.rmngp.fr/},
  urldate = {2023-08-10},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/VAAJ88VF/api.art.rmngp.fr.html}
}

@book{apollonDigitalCriticalEditions2014,
  title = {{Digital Critical Editions}},
  author = {Apollon, Daniel and Belisle, Claire and Regnier, Philippe},
  year = {2014},
  month = jun,
  publisher = {{University of Illinois Press}},
  address = {{Urbana Chicaco Sprinfgield}},
  abstract = {Provocative yet sober,  Digital Critical Editions examines how transitioning from print to a digital milieu deeply affects how scholars deal with the work of editing critical texts. On one hand, forces like changing technology and evolving reader expectations lead to the development of specific editorial products, while on the other hand, they threaten traditional forms of knowledge and methods of textual scholarship.Using the experiences of philologists, text critics, text encoders, scientific editors, and media analysts, Digital Critical Editions ranges from philology in ancient Alexandria to the vision of user-supported online critical editing, from peer-directed texts distributed to a few to community-edited products shaped by the many. The authors discuss the production and accessibility of documents, the emergence of tools used in scholarly work, new editing regimes, and how the readers' expectations evolve as they navigate digital texts. The goal: exploring questions such as, What kind of text is produced? Why is it produced in this particular way?Digital Critical Editions provides digital editors, researchers, readers, and technological actors with insights for addressing disruptions that arise from the clash of traditional and digital cultures, while also offering a practical roadmap for processing traditional texts and collections with today's state-of-the-art editing and research techniques thus addressing readers' new emerging reading habits.},
  isbn = {978-0-252-03840-2},
  langid = {Anglais},
  keywords = {DH}
}

@misc{arslanCreateMachineLearning2021,
  title = {Create a {{Machine Learning API With Django Rest Framework}} | {{Better Programming}}},
  author = {Arslan, G{\"o}rkem},
  year = {2021},
  month = jul,
  url = {https://betterprogramming.pub/create-a-machine-learning-api-with-django-rest-framework-967571640c46},
  urldate = {2023-08-27},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/TCB9CHD3/create-a-machine-learning-api-with-django-rest-framework-967571640c46.html}
}

@book{azencottIntroductionAuMachine2018,
  title = {{Introduction au Machine Learning}},
  author = {Azencott, Chlo{\'e}-Agathe},
  year = {2018},
  month = sep,
  publisher = {{Dunod}},
  address = {{Malakoff}},
  abstract = {Cet ouvrage s'adresse aux \'etudiants en fin de licence et en master d'informatique ou de maths appliqu\'ees, ainsi qu'aux \'el\`eves ing\'enieurs. Le Machine Learning est une discipline dont les outils puissants permettent aujourd'hui \`a de nombreux secteurs d'activit\'e de r\'ealiser des progr\`es spectaculaires gr\^ace \`a l'exploitation de grands volumes de donn\'ees. Le but de cet ouvrage est de vous fournir des bases solides sur les concepts et les algorithmes de ce domaine en plein essor. Il vous aidera \`a identifier les probl\`emes qui peuvent \^etre r\'esolus par une approche Machine Learning, \`a les formaliser, \`a identifier les algorithmes les mieux adapt\'es \`a chaque probl\`eme, \`a les mettre en oeuvre, et enfin \`a savoir \'evaluer les r\'esultats obtenus. Les notions de cours sont illustr\'ees et compl\'et\'ees par 86 exercices, tous corrig\'es.},
  isbn = {978-2-10-078080-8},
  langid = {french},
  keywords = {ml}
}

@article{baujardNumerisationPatrimoineCulturel2017,
  title = {{Num\'erisation du patrimoine culturel et strat\'egie manag\'eriale des mus\'ees}},
  author = {Baujard, Corinne},
  year = {2017},
  month = feb,
  journal = {Management des technologies organisationnelles},
  number = {7},
  pages = {69--78},
  publisher = {{Les Presses des Mines}},
  issn = {2553-3851},
  url = {https://www.cairn.info/revue-management-des-technologies-organisationnelles-2017-2-page-69.htm},
  urldate = {2023-08-06},
  abstract = {Ce septi\`eme volume de la revue MTO pr\'esente la particularit\'e de r\'eunir des textes centr\'es sur le th\`eme \guillemotleft ~d\'esordres num\'eriques~: incertitude et opportunit\'es~\guillemotright{} (dans la continuit\'e du volume 06), et des textes centr\'es sur le th\`eme \guillemotleft ~engagement des publics et des entreprises dans un environnement dynamique~: le r\^ole des r\'eseaux mondiaux~\guillemotright.Nous aurions pu organiser ce num\'ero en deux parties, mais il nous a sembl\'e plus int\'eressant de rassembler les textes sous une banni\`ere commune \guillemotleft ~num\'erique et organisations~\guillemotright{} tout en conservant l'organisation de notre revue en trois parties~: le point de vue de scientifiques de renom en cartes blanches, les contributions au th\`eme d\'evelopp\'e dans le volume, et un ensemble de textes issus du terrain.Bien entendu, nous conservons la particularit\'e de notre revue qui est de donner la parole aux chercheurs exp\'eriment\'es et aux chercheurs en devenir tels que les doctorants, ainsi qu'aux consultants et aux acteurs en entreprises. Dans tous les cas l'objectif est le m\^eme~: privil\'egier l'apport d'id\'ees nouvelles pour renforcer la comp\'etitivit\'e des entreprises},
  langid = {french},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/PLQXSA74/revue-management-des-technologies-organisationnelles-2017-2-page-69.html}
}

@article{benhamouDroitAuteurMusees2016,
  title = {{Droit d'auteur et mus\'ees num\'eriques}},
  author = {Benhamou, Yaniv},
  year = {2016},
  month = jun,
  journal = {Magazine de l'OMPI},
  number = {3},
  url = {https://www.wipo.int/wipo_magazine/fr/2016/03/article_0005.html},
  urldate = {2023-08-06},
  langid = {french},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/DDD977ER/article_0005.html}
}

@article{bergstromImageQualityComputer2023,
  title = {Image Quality and Computer Vision Performance: Assessing the Effects of Image Distortions and Modeling Performance Relationships Using the General Image Quality Equation},
  shorttitle = {Image Quality and Computer Vision Performance},
  author = {Bergstrom, Austin C. and Messinger, David W.},
  year = {2023},
  month = mar,
  journal = {Journal of Electronic Imaging},
  volume = {32},
  pages = {023018},
  issn = {1017-9909},
  doi = {10.1117/1.JEI.32.2.023018},
  url = {https://ui.adsabs.harvard.edu/abs/2023JEI....32b3018B},
  urldate = {2023-08-07},
  abstract = {Substantial research has explored methods to optimize convolutional neural networks (CNNs) for tasks such as image classification and object detection, but research into the image quality drivers of computer vision performance has been limited. Additionally, there are indications that image degradations such as blur and noise affect human visual interpretation and machine interpretation differently. The general image quality equation (GIQE) predicts overhead image quality for human analysis using the National Image Interpretability Rating Scale, but no such model exists to predict image quality for interpretation by CNNs. Here, we assess the relationship between image quality variables and CNN performance. Specifically, we examine the impacts of resolution, blur, and noise on CNN performance for models trained with in-distribution and out-of-distribution distortions. Using two datasets, we observe that while generalization remains a significant challenge for CNNs faced with out-of-distribution image distortions, CNN performance against low visual quality images remains strong with appropriate training, indicating the potential to expand the design trade space for sensors providing data to computer vision systems. Additionally, we find that CNN performance predictions using the functional form of the GIQE can predict CNN performance as a function of image degradation, but we observe that the legacy form of the GIQE (from GIQE versions 3 and 4) does a better job of modeling the impact of blur/relative edge response in our experiments.},
  keywords = {ml},
  annotation = {ADS Bibcode: 2023JEI....32b3018B}
}

@book{besseNumerisationMasseVers2019,
  title = {Num\'erisation de Masse : Vers La Cr\'eation d'un Nouvel Acteur de l'information. {{Le}} Projet {{Time Machine}}},
  author = {Besse, Camille},
  year = {2019},
  publisher = {{\'Ecole nationale des chartes}},
  address = {{Paris}},
  keywords = {technique}
}

@misc{bishopDigitalArtHistory2017,
  type = {Text},
  title = {Against {{Digital Art History}}},
  author = {Bishop, Claire},
  year = {2017},
  month = mar,
  journal = {Franklin Humanities Institute},
  publisher = {{Franklin Humanities Institute}},
  url = {https://humanitiesfutures.org/papers/digital-art-history/},
  urldate = {2023-08-01},
  abstract = {This article responds to two issues affecting the field of contemporary art history: digital technology and the so-called computational turn in the humanities.},
  copyright = {Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License},
  langid = {english},
  keywords = {DH},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/VN2ENVVH/digital-art-history.html}
}

@article{brachmannComputationalExperimentalApproaches2017,
  title = {Computational and {{Experimental Approaches}} to {{Visual Aesthetics}}},
  author = {Brachmann, Anselm and Redies, Christoph},
  year = {2017},
  journal = {Frontiers in Computational Neuroscience},
  volume = {11},
  issn = {1662-5188},
  url = {https://www.frontiersin.org/articles/10.3389/fncom.2017.00102},
  urldate = {2023-08-01},
  abstract = {Aesthetics has been the subject of long-standing debates by philosophers and psychologists alike. In psychology, it is generally agreed that aesthetic experience results from an interaction between perception, cognition, and emotion. By experimental means, this triad has been studied in the field of experimental aesthetics, which aims to gain a better understanding of how aesthetic experience relates to fundamental principles of human visual perception and brain processes. Recently, researchers in computer vision have also gained interest in the topic, giving rise to the field of computational aesthetics. With computing hardware and methodology developing at a high pace, the modeling of perceptually relevant aspect of aesthetic stimuli has a huge potential. In this review, we present an overview of recent developments in computational aesthetics and how they relate to experimental studies. In the first part, we cover topics such as the prediction of ratings, style and artist identification as well as computational methods in art history, such as the detection of influences among artists or forgeries. We also describe currently used computational algorithms, such as classifiers and deep neural networks. In the second part, we summarize results from the field of experimental aesthetics and cover several isolated image properties that are believed to have a effect on the aesthetic appeal of visual stimuli. Their relation to each other and to findings from computational aesthetics are discussed. Moreover, we compare the strategies in the two fields of research and suggest that both fields would greatly profit from a joined research effort. We hope to encourage researchers from both disciplines to work more closely together in order to understand visual aesthetics from an integrated point of view.},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/4DKWHUWL/Brachmann et Redies - 2017 - Computational and Experimental Approaches to Visua.pdf}
}

@article{brandhorstAbyWarburgWildest2013,
  title = {Aby {{Warburg}}'s Wildest Dreams Come True?},
  author = {Brandhorst, Hans},
  year = {2013},
  month = jun,
  journal = {Visual Resources},
  volume = {29},
  number = {1-2},
  pages = {72--88},
  issn = {0197-3762},
  doi = {10.1080/01973762.2013.761129},
  url = {https://repub.eur.nl/pub/40363},
  urldate = {2023-08-01},
  abstract = {In the midst of the "digital revolution," should we be pleased or disappointed about its effects in the field of art history? How does one answer this question, which is most likely premature and lacking in analytical acuity given what we know about art historical preferences and practices? But even if we accept the digital revolution as a broad concept to mean simply that image researchers have an unprecedented flood of reproductions at their fingertips, there is no question that we can still detect real feelings of unease about what this flood has brought us so far. This article takes a simple visual motif-one person carrying another-as a starting point for an iconographic exploration. In demonstrating and imagining how source material could be collected, arranged, and rearranged, it recalls in a general way the panels with photographs that Aby Warburg (1866-1929) set up to help him develop ideas and relationships in the process of his art historical research. If we could design easily accessible corpora of sources so that continuous rearrangement by visual motif and meaning were easy, and if we could seduce researchers and catalogers to cooperate in enriching them with more and better descriptions, tags, and links, the study of iconography could make a significant leap forward.},
  langid = {english},
  keywords = {DH},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/RWH8BY6L/Brandhorst - 2013 - Aby Warburg's wildest dreams come true.pdf}
}

@misc{brossaultQuEstceQu,
  title = {{Qu'est-ce qu'une API REST et quel est l'int\'er\^et ?}},
  author = {Brossault, B{\'e}n{\'e}dicte},
  url = {https://blog.hubspot.fr/website/api-rest},
  urldate = {2023-08-27},
  abstract = {Gr\^ace \`a une API REST, il est possible pour des logiciels qui n'utilisent pas le m\^eme syst\`eme d'exploitation d'interagir et de partager des informations.},
  langid = {fr, fr},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/LRJJICKF/api-rest.html}
}

@article{buttnerCorDeepSacroboscoDataset2022,
  title = {{{CorDeep}} and the {{Sacrobosco Dataset}}: {{Detection}} of {{Visual Elements}} in {{Historical Documents}}},
  shorttitle = {{{CorDeep}} and the {{Sacrobosco Dataset}}},
  author = {B{\"u}ttner, Jochen and Martinetz, Julius and {El-Hajj}, Hassan and Valleriani, Matteo},
  year = {2022},
  month = oct,
  journal = {Journal of Imaging},
  volume = {8},
  number = {10},
  pages = {285},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2313-433X},
  doi = {10.3390/jimaging8100285},
  url = {https://www.mdpi.com/2313-433X/8/10/285},
  urldate = {2023-08-10},
  abstract = {Recent advances in object detection facilitated by deep learning have led to numerous solutions in a myriad of fields ranging from medical diagnosis to autonomous driving. However, historical research is yet to reap the benefits of such advances. This is generally due to the low number of large, coherent, and annotated datasets of historical documents, as well as the overwhelming focus on Optical Character Recognition to support the analysis of historical documents. In this paper, we highlight the importance of visual elements, in particular illustrations in historical documents, and offer a public multi-class historical visual element dataset based on the Sphaera corpus. Additionally, we train an image extraction model based on YOLO architecture and publish it through a publicly available web-service to detect and extract multi-class images from historical documents in an effort to bridge the gap between traditional and computational approaches in historical studies.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/WLAD65M9/Büttner et al. - 2022 - CorDeep and the Sacrobosco Dataset Detection of V.pdf}
}

@article{cannyComputationalApproachEdge1986,
  title = {A {{Computational Approach}} to {{Edge Detection}}},
  author = {Canny, John},
  year = {1986},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-8},
  number = {6},
  pages = {679--698},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1986.4767851},
  abstract = {This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.},
  keywords = {ml}
}

@misc{carremansHandlingOverfittingDeep2019,
  title = {Handling Overfitting in Deep Learning Models},
  author = {Carremans, Bert},
  year = {2019},
  month = jan,
  journal = {Medium},
  url = {https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e},
  urldate = {2023-08-22},
  abstract = {Overfitting occurs when you achieve a good fit of your model on the training data, while it does not generalize well on new, unseen data\ldots},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/69537YEB/handling-overfitting-in-deep-learning-models-c760ee047c6e.html}
}

@misc{CategoryGoogleArt,
  title = {Category:{{Google Art Project}} Works by Collection - {{Wikimedia Commons}}},
  shorttitle = {Category},
  url = {https://commons.wikimedia.org/wiki/Category:Google_Art_Project_works_by_collection},
  urldate = {2023-08-10},
  langid = {english},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/C8EQ8JDT/CategoryGoogle_Art_Project_works_by_collection.html}
}

@misc{CeleryDistributedTask,
  title = {Celery - {{Distributed Task Queue}} \textemdash{} {{Celery}} 5.3.1 Documentation},
  url = {https://docs.celeryq.dev/en/stable/},
  urldate = {2023-08-29},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/QS2ALGNA/stable.html}
}

@misc{chagueHTRUnited,
  title = {{{HTR-United}}},
  author = {Chagu{\'e}, Alix and Cl{\'e}rice, Thibault},
  url = {https://htr-united.github.io/index.html},
  urldate = {2023-08-10},
  abstract = {HTR-United is a catalog and an ecosystem for sharing and finding ground truth for optical character or handwritten text recognition (OCR/HTR).},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/Q4NIWBLZ/index.html}
}

@book{cholletApprentissageProfondAvec2020a,
  title = {{L'apprentissage profond avec Python}},
  author = {Chollet, Fran{\c c}ois},
  year = {2020},
  month = jul,
  publisher = {{Machinelearning.fr}},
  address = {{Saint-Cyr-sur-Loire}},
  abstract = {L'apprentissage automatique a fait des progr\`es remarquables au cours des derni\`eres ann\'ees. Nous sommes pass\'es d'une reconnaissance quasi inutilisable de la parole et des images \`a une pr\'ecision quasi humaine, de machines qui ne pouvaient pas battre un joueur de Go un peu exp\'eriment\'e \`a la d\'efaite d'un champion du monde. Derri\`ere ces progr\`es se cache l'apprentissage profond \rule{1em}{1pt} une combinaison d'avanc\'ees th\'eoriques et pratiques qui permet une multitude d'applications intelligentes jusque-l\`a impossibles \`a r\'ealiser.L'apprentissage profond avec Python pr\'esente le domaine de l'apprentissage profond en utilisant le langage Python et la puissante biblioth\`eque Keras. R\'edig\'e par Fran\c{c}ois Chollet, cr\'eateur de Keras et chercheur en intelligence artificielle \`a Google, cet ouvrage construit votre compr\'ehension de l'apprentissage profond gr\^ace \`a des explications intuitives et des exemples pratiques. Vous explorerez des concepts et des pratiques stimulantes avec des applications de vision par ordinateur, de traitement du langage naturel et de mod\`eles g\'en\'eratifs. Lorsque vous aurez termin\'e, vous aurez les connaissances et les comp\'etences pratiques n\'ecessaires pour appliquer l'apprentissage profond \`a vos propres projets.Aucune exp\'erience pr\'ealable de Keras, de TensorFlow ou d'apprentissage automatique n'est requise. Les lecteurs ont besoin de comp\'etences Python interm\'ediaires.Le livre est imprim\'e sur du papier certifi\'e PEFC (Programme for the Endorsement of Forest Certification), les encres et techniques d'impression ont le label Imprim'Vert. Le Livre est imprim\'e en R\'egion Centre-Val de Loire. Une compensation volontaire carbone de 3kg par ouvrage (pesant 1,250kg) est vers\'ee \`a la Fondation GoodPlanet.org. Machinelearning.fr s'engage pour la transition environnementale.},
  isbn = {978-2-491-67400-7},
  langid = {french},
  keywords = {ml}
}

@incollection{claerrModeEmploi2017,
  title = {{Mode d'emploi}},
  booktitle = {{Num\'eriser et mettre en ligne}},
  author = {Claerr, Thierry and Westeel, Isabelle},
  year = {2017},
  series = {{La Bo\^ite \`a outils}},
  pages = {7--18},
  publisher = {{Presses de l'enssib}},
  address = {{Villeurbanne}},
  doi = {10.4000/books.pressesenssib.419},
  url = {http://books.openedition.org/pressesenssib/419},
  urldate = {2023-08-06},
  copyright = {https://www.openedition.org/12554},
  isbn = {978-2-37546-032-0},
  langid = {french},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/TS4BD2WB/Claerr et Westeel - 2017 - Mode d’emploi.pdf}
}

@unpublished{clericeYouActuallyLook2023,
  title = {You {{Actually Look Twice At}} It ({{YALTAi}}): Using an Object Detection Approach Instead of Region Segmentation within the {{Kraken}} Engine},
  shorttitle = {You {{Actually Look Twice At}} It ({{YALTAi}})},
  author = {Cl{\'e}rice, Thibault},
  year = {2023},
  month = apr,
  url = {https://enc.hal.science/hal-03723208},
  urldate = {2023-08-15},
  abstract = {Layout Analysis (the identification of zones and their classification) is the first step along line segmentation in Optical Character Recognition and similar tasks. The ability of identifying main body of text from marginal text or running titles makes the difference between extracting the work full text of a digitized book and noisy outputs. We show that most segmenters focus on pixel classification and that polygonization of this output has not been used as a target for the latest competition on historical document (ICDAR 2017 and onwards), despite being the focus in the early 2010s. We propose to shift, for efficiency, the task from a pixel classification-based polygonization to an object detection using isothetic rectangles. We compare the output of Kraken and YOLOv5 in terms of segmentation and show that the later severely outperforms the first on small datasets (1110 samples and below). We release two datasets for training and evaluation on historical documents as well as a new package, YALTAi, which injects YOLOv5 in the segmentation pipeline of Kraken 4.1.},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/ZZHHWQ4D/Clérice - 2023 - You Actually Look Twice At it (YALTAi) using an o.pdf}
}

@misc{COCOCommonObjects,
  title = {{{COCO}} - {{Common Objects}} in {{Context}}},
  url = {https://cocodataset.org/#home},
  urldate = {2023-08-15},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/7P54XWHA/cocodataset.org.html}
}

@misc{CorDeep,
  title = {{{CorDeep}}},
  url = {https://cordeep.mpiwg-berlin.mpg.de/},
  urldate = {2023-08-10}
}

@misc{Corpus,
  title = {Corpus},
  journal = {VHS project},
  url = {https://vhs.hypotheses.org/corpus},
  urldate = {2023-08-09},
  abstract = {The work we will be carried out on on four original corpora selected first for their relevance to the problem of the circulation of scientific knowledge through illustration, and second for their chronological, geographical...},
  langid = {american},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/NL7LSJDJ/corpus.html}
}

@misc{cramerInternationalImageInteroperability2011,
  title = {The {{International Image Interoperability Framework}} ({{IIIF}}): {{Laying}} the {{Foundation}} for {{Common Services}}, {{Integrated Resources}} and a {{Marketplace}} of {{Tools}} for {{Scholars Worldwide}}},
  shorttitle = {The {{International Image Interoperability Framework}} ({{IIIF}})},
  author = {Cramer, Tom},
  year = {2011},
  month = dec,
  journal = {CNI: Coalition for Networked Information},
  url = {https://www.cni.org/topics/information-access-retrieval/international-image-interoperability-framework},
  urldate = {2023-08-08},
  langid = {american},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/7ZAC8MP7/international-image-interoperability-framework.html}
}

@article{Cron2023,
  title = {{cron}},
  year = {2023},
  month = jul,
  journal = {Wikip\'edia},
  url = {https://fr.wikipedia.org/w/index.php?title=Cron&oldid=206272232},
  urldate = {2023-08-29},
  abstract = {cron est un programme qui permet aux utilisateurs des syst\`emes Unix d'ex\'ecuter automatiquement des scripts, des commandes ou des logiciels \`a une date et une heure sp\'ecifi\'ee \`a l'avance, ou selon un cycle d\'efini \`a l'avance.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french},
  annotation = {Page Version ID: 206272232},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/YGRDKKBV/Cron.html}
}

@inproceedings{crowleyArtDetection2016,
  title = {The {{Art}} of {{Detection}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016 {{Workshops}}},
  author = {Crowley, Elliot J. and Zisserman, Andrew},
  editor = {Hua, Gang and J{\'e}gou, Herv{\'e}},
  year = {2016},
  volume = {9913},
  pages = {721--737},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46604-0_50},
  url = {http://link.springer.com/10.1007/978-3-319-46604-0_50},
  urldate = {2023-08-01},
  abstract = {The objective of this work is to recognize object categories in paintings, such as cars, cows and cathedrals. We achieve this by training classifiers from natural images of the objects. We make the following contributions: (i) we measure the extent of the domain shift problem for image-level classifiers trained on natural images vs paintings, for a variety of CNN architectures; (ii) we demonstrate that classification-by-detection (i.e. learning classifiers for regions rather than the entire image) recognizes (and locates) a wide range of small objects in paintings that are not picked up by image-level classifiers, and combining these two methods improves performance; and (iii) we develop a system that learns a region-level classifier on-the-fly for an object category of a user's choosing, which is then applied to over 60 million object regions across 210,000 paintings to retrieve localised instances of that category.},
  isbn = {978-3-319-46603-3 978-3-319-46604-0},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/QX3BSFBX/Crowley et Zisserman - 2016 - The Art of Detection.pdf}
}

@misc{DarknetOpenSource,
  title = {Darknet: {{Open Source Neural Networks}} in {{C}}},
  url = {https://pjreddie.com/darknet/},
  urldate = {2023-08-18},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/DX33BYDG/darknet.html}
}

@misc{denoyelleProposCoutImages2021,
  type = {{Billet}},
  title = {{A propos du co\^ut des images d'\oe uvres patrimoniales}},
  author = {Denoyelle, Martine},
  year = {2021},
  month = jun,
  journal = {Num\'erique et recherche en histoire de l'art},
  url = {https://numrha.hypotheses.org/2142},
  urldate = {2023-08-06},
  abstract = {Ce billet est publi\'e sur le carnet Numrha dans le cadre de la parution du~guide pratique pour la recherche et la r\'eutilisation des images d'oeuvres d'art.~Le pr\'esent billet compl\`ete les...},
  langid = {french},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/RP5HXRX4/2142.html}
}

@misc{denoyelleSavoirLicenceEtalab2021,
  type = {{Billet}},
  title = {{En savoir plus sur la licence Etalab et les autres licences}},
  author = {Denoyelle, Martine},
  year = {2021},
  month = jun,
  journal = {Num\'erique et recherche en histoire de l'art},
  url = {https://numrha.hypotheses.org/2134},
  urldate = {2023-08-06},
  abstract = {Ce billet est publi\'e sur le carnet~Numrha~dans le cadre de la parution du~guide pratique pour la recherche et la r\'eutilisation des images d'oeuvres d'art.~Le pr\'esent billet compl\`ete le chapitre 3...},
  langid = {french},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/7G5RQ8KB/2134.html}
}

@misc{DeploymentProductiona,
  title = {Deployment to Production},
  journal = {GitHub},
  url = {https://github.com/jnorindr/extractorAPI/wiki/Deployment-to-production},
  urldate = {2023-09-01},
  abstract = {API to run inference on a GPU using a vision model for element extraction - jnorindr/extractorAPI},
  langid = {english},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/ATAZY6MU/Deployment-to-production.html}
}

@article{derpanisOverviewRANSACAlgorithm2010a,
  title = {Overview of the {{RANSAC Algorithm}}},
  author = {Derpanis, Konstantinos G},
  year = {2010},
  month = may,
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/AFNNHFEY/Derpanis - Overview of the RANSAC Algorithm.pdf}
}

@article{deyoungEditingCollectionDiagrams2014,
  title = {Editing a {{Collection}} of {{Diagrams Ascribed}} to {{Al-\d{H}ajj\=aj}}: {{An Initial Case Study}}},
  author = {De Young, Gregg},
  year = {2014},
  journal = {SCIAMVS},
  volume = {15},
  pages = {171--238},
  url = {https://www.sciamvs.org/files/SCIAMVS_15_171-238_DeYoung.pdf},
  keywords = {astronomie}
}

@misc{DigitalImages2020,
  title = {Digital {{Images}}},
  year = {2020},
  url = {https://livecode.byu.edu/images/DigitalImages.php},
  urldate = {2023-08-07},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/DNQAAKS8/DigitalImages.html}
}

@misc{Django,
  title = {Django},
  journal = {Django Project},
  url = {https://www.djangoproject.com/},
  urldate = {2023-08-27},
  abstract = {The web framework for perfectionists with deadlines.},
  langid = {english},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/TTGCXMLL/www.djangoproject.com.html}
}

@misc{DjangoRESTFramework,
  title = {Django {{REST}} Framework},
  url = {https://www.django-rest-framework.org/},
  urldate = {2023-08-27},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/UQSXIBW7/www.django-rest-framework.org.html}
}

@misc{donovanHowAIHelping2023,
  title = {How {{AI}} Is Helping Historians Better Understand Our Past},
  author = {Donovan, Moira},
  year = {2023},
  month = apr,
  journal = {MIT Technology Review},
  url = {https://www.technologyreview.com/2023/04/11/1071104/ai-helping-historians-analyze-past/},
  urldate = {2023-08-10},
  abstract = {The historians of tomorrow are using computer science to analyze how people lived centuries ago.},
  langid = {english},
  keywords = {DH},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/U94RDPUQ/ai-helping-historians-analyze-past.html}
}

@misc{dotTableTranscriberAutomaticPipeline2022,
  title = {{{tableTranscriber}}: An Automatic Pipeline for Astronomical Tables Transcription},
  shorttitle = {{{tableTranscriber}}},
  author = {Dot, Tristan},
  year = {2022},
  month = oct,
  url = {https://github.com/tristandot/tableTranscriber},
  urldate = {2023-08-10},
  copyright = {MIT}
}

@misc{dowlingGuideFileFormats2019,
  title = {Guide to {{File Formats}} for {{Machine Learning}}: {{Columnar}}, {{Training}}, {{Inferencing}}, and the {{Feature Store}}},
  shorttitle = {Guide to {{File Formats}} for {{Machine Learning}}},
  author = {Dowling, Jim},
  year = {2019},
  month = oct,
  journal = {Medium},
  url = {https://towardsdatascience.com/guide-to-file-formats-for-machine-learning-columnar-training-inferencing-and-the-feature-store-2e0c3d18d4f9},
  urldate = {2023-08-29},
  abstract = {TLDR; Most machine learning models are trained using data from files. This post is a guide to the popular file formats used in open source\ldots},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/7TX4HSZ3/guide-to-file-formats-for-machine-learning-columnar-training-inferencing-and-the-feature-store-.html}
}

@book{eastwoodPlanetaryDiagramsRoman2004,
  title = {Planetary Diagrams for {{Roman}} Astronomy in Medieval {{Europe}}, ca. 800-1500},
  author = {Eastwood, Bruce S.},
  year = {2004},
  publisher = {{American Philosophical Society}},
  address = {{Philadelphia}},
  keywords = {astronomie}
}

@book{eastwoodRevivalPlanetaryAstronomy2018,
  title = {The Revival of Planetary Astronomy in {{Carolingian}} and Post-{{Carolingian Europe}}},
  author = {Eastwood, Bruce S.},
  year = {2018},
  series = {Routledge Revivals;  {{Variorum}} Collected Studies Series ,  {{CS729}}},
  edition = {[Facsimile ed},
  publisher = {{Routledge}},
  address = {{London New York}},
  isbn = {978-1-138-73192-9},
  langid = {english},
  lccn = {Tolbiac - Rez de Jardin - Sciences et technique - Salle R - Libre acc\`es - 520.901 EAST r},
  nationality = {GB},
  keywords = {astronomie},
  annotation = {ill. 23 cm. Recueil d'articles parus dans diverses revues et publications entre 1983 et 2000. Notes bibliogr.}
}

@misc{EdIterAnalyserDiagrammes,
  title = {{EdIter et analyser les Diagrammes astronomiques historiques avec l'intelligence Artificielle}},
  journal = {Agence nationale de la recherche},
  url = {https://anr.fr/Projet-ANR-22-CE38-0014},
  urldate = {2023-08-10},
  abstract = {Pendant des si\`ecles, dans toute l'Eurasie, les sciences astronomiques ont \'et\'e d\'evelopp\'ees, pour r\'epondre \`a un large \'eventail de besoins religieux, rituels et politiques, pour observer et comprendre le monde naturel. Par cons\'equent, dans de multiples contextes culturels interd\'ependants, les praticiens des sciences astronomiques ont compil\'e des manuscrits, dont il existe des centaines de milliers, qui comprennent des tables num\'eriques, des textes et des diagrammes. EIDA se concentrera sur les diagrammes en tant que patrimoine visuel et v\'ehicule d'arguments astronomiques et math\'ematiques. \`A cette fin, le projet d\'eveloppera et mettra en \oe uvre une approche radicalement nouvelle dans l'\'etude des diagrammes astronomiques des p\'eriodes modernes et pr\'emodernes. Les deux principaux objectifs de EIDA sont : (1) la prise en compte de la vari\'et\'e des fonctions et des modes de circulation des diagrammes dans l'histoire de l'astronomie en analysant conjointement leurs aspects \'epist\'emiques et documentaires dans les sciences astrales (2) le d\'eveloppement de nouvelles approches de vision par ordinateur, capables de d\'ecomposer un diagramme astronomique en composantes significatives pour l'analyse et l'\'edition, sans d\'ependre d'une annotation humaine. EIDA associe les progr\`es r\'ecents des approches d'analyse par synth\`ese en vision par ordinateur et le r\'ecent tournant visuel en histoire de l'astronomie dans une perspective interdisciplinaire. Cette collaboration arrive \`a point nomm\'e compte tenu de la disponibilit\'e des donn\'ees et permettra de mener des analyses fines \`a une \'echelle sans pr\'ec\'edent ouvrant de nouveaux horizons pour l'histoire de l'astronomie. En outre, conform\'ement aux principes de la science ouverte, EIDA d\'eveloppera des outils permettant de moissonner, d'\'etudier, d'\'editer et de visualiser des diagrammes par le biais d'une interface web, sur le mod\`ele de la plateforme DISHAS pour les tables astronomiques.},
  langid = {french},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/29IHH6RD/Projet-ANR-22-CE38-0014.html}
}

@misc{EnhancingHeritageImage,
  title = {Enhancing {{Heritage Image Databases}}},
  journal = {Agence nationale de la recherche},
  url = {https://anr.fr/Project-ANR-17-CE23-0008},
  urldate = {2023-08-10},
  abstract = {In recent years, computer vision has made several breakthroughs by using very large databases to train deep Convolutional Neural Networks (CNNs). In parallel, a lot of efforts have been invested to digitalize heritage artifacts, such as museum collections or archive images, that are now publicly accessible. CNNs have been successfully tested on these heritage data, but mainly for standard classification tasks, inside a closed database. On the contrary, this project aims at using of the most recent advances in computer vision, and in particular in deep learning, to develop innovative applications that are made possible by the availability of large databases. In particular, we will target tasks such as invariant pattern discovery in large image databases and 3D reconstruction from historical depictions.{$<$}br /{$><$}br /{$>$}Breakthrough progress on these problems would have profound implications both in human sciences such as art history and archaeology, and in their understanding by the public.},
  langid = {english},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/MQWME4R4/Project-ANR-17-CE23-0008.html}
}

@misc{Europeana,
  title = {{Europeana}},
  url = {https://www.europeana.eu/fr},
  urldate = {2023-08-10},
  abstract = {Recherchez, enregistrez et partagez des \oe uvres d'art, des livres, des films et de la musique issues de milliers d'institutions culturelles},
  langid = {french},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/LG49TJC2/fr.html}
}

@incollection{evansHistoryAstronomy,
  title = {History of {{Astronomy}}},
  booktitle = {Encyclopedia {{Britannica}}},
  author = {Evans, James},
  url = {https://www.britannica.com/science/astronomy/Ancient-Greece},
  urldate = {2023-08-03},
  abstract = {astronomy - Astronomy - Ancient Greece, Stars, Planets: Astronomy is present from the beginning of Greek literature. In Homer's Iliad and Odyssey, stars and constellations are mentioned, including Orion, the Great Bear (Ursa Major), Bo\"otes, Sirius, and the Pleiades. More-detailed astronomical knowledge is found in Hesiod's Works and Days, from perhaps a generation later than Homer. Hesiod used the appearances and disappearances of important fixed stars in the course of the annual cycle in order to prescribe the work to be done around the farm or the seasons for safe sailing. Much of the astronomical knowledge in Hesiod paralleled the knowledge of the contemporary Babylonians, but the Greeks},
  langid = {english},
  keywords = {astronomie},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/3GQIBHNR/Ancient-Greece.html}
}

@misc{ExtractorAPIWiki2023,
  title = {{{extractorAPI}} Wiki},
  url = {https://github.com/jnorindr/extractorAPI/wiki},
  urldate = {2023-09-01},
  abstract = {API to run inference on a GPU using a vision model for element extraction}
}

@article{falomirCategorizingPaintingsArt2018,
  title = {Categorizing Paintings in Art Styles Based on Qualitative Color Descriptors, Quantitative Global Features and Machine Learning ({{QArt-Learn}})},
  author = {Falomir, Zoe and Museros, Lled{\'o} and Sanz, Ismael and {Gonzalez-Abril}, Luis},
  year = {2018},
  month = may,
  journal = {Expert Systems with Applications},
  volume = {97},
  pages = {83--94},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2017.11.056},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417417308126},
  urldate = {2023-08-01},
  abstract = {The QArt-Learn approach for style painting categorization based on Qualitative Color Descriptors (QCD), color similarity (SimQCD), and quantitative global features (i.e. average of brightness, hue, saturation and lightness and brightness contrast) is presented in this paper. k-Nearest Neighbor (k-NN) and support vector machine (SVM) techniques have been used for learning the features of paintings from the Baroque, Impressionism and Post-Impressionism styles. Specifically two classifiers are built, and two different parameterizations have been applied for the QCD. For testing QArt-Learn approach, the Painting-91 dataset has been used, from which the paintings corresponding to Vel\'azquez, Vermeer, Monet, Renoir, van Gogh and Gauguin were extracted, resulting in a set of 252 paintings. The results obtained have shown categorization accuracies higher than 65\%, which are comparable to accuracies obtained in the literature. However, QArt-Learn uses qualitative color names which can describe style color palettes linguistically, so that they can be better understood by non-experts in art since QCDs are aligned with human perception.},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/98JET9D2/S0957417417308126.html}
}

@misc{FastAPI,
  title = {{{FastAPI}}},
  url = {https://fastapi.tiangolo.com/},
  urldate = {2023-08-27},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/5KGJM6ZS/fastapi.tiangolo.com.html}
}

@misc{FastAPIFrameworkPython2023,
  title = {{FastAPI, le framework python alternatif \`a Django et Flask ?}},
  year = {2023},
  month = jun,
  journal = {Cr\'eateur de site internet - agence web WordPress},
  url = {https://www.agencedebord.com/blog/fastapi-le-framework-python-alternatif-a-django-et-flask},
  urldate = {2023-08-27},
  abstract = {Si vous \'evoquez d\'eveloppement web backend \`a notre \'equipe, il se peut que Django soit le premier mot que vous entendiez. Ce framework python nous permet en effet de mettre en place des projets importants tout en le couplant avec une application frontend. D'ailleurs Django est notre solution de pr\'edilection d\`es que nous voulons d\'evelopper [\ldots ]},
  langid = {french},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/VDRFX8XX/fastapi-le-framework-python-alternatif-a-django-et-flask.html}
}

@misc{FlaskDocumentation,
  title = {Flask {{Documentation}} (2.3.x)},
  url = {https://flask.palletsprojects.com/en/2.3.x/},
  urldate = {2023-08-27},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/FPNJ6TLJ/2.3.x.html}
}

@inproceedings{fokaComputerVisionApplications2021,
  title = {Computer {{Vision Applications}} for {{Art History}}: {{Reflections}} and Paradigms for Future Research},
  shorttitle = {Computer {{Vision Applications}} for {{Art History}}},
  booktitle = {Proceedings of {{EVA London}} 2021},
  author = {Foka, Amalia F.},
  year = {2021},
  month = jul,
  pages = {73--80},
  publisher = {{BCS Learning \& Development}},
  doi = {10.14236/ewic/EVA2021.12},
  url = {https://www.scienceopen.com/hosted-document?doi=10.14236/ewic/EVA2021.12},
  urldate = {2023-08-01},
  abstract = {One of the contributing factors to the continuing debate among art historians over the use of computational methods in art history research is that they do not consider the core of today's art history research questions. The lack of close collaboration between the two involved research communities makes the definition of contemporary art-historical methods as well-defined computer vision problems extremely difficult. For that purpose, it is devised as a methodology to study articles in art history journals from a computer science perspective. The objective is to identify which image features art historians utilise within their research and describe them in immediate and meaningful terms to the computer vision research community. Finally, some paradigms that could serve as a new starting point for exploring how computer vision applications for art history can address the core of today's art history research are given.},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/LX3GPZTH/Foka - 2021 - Computer Vision Applications for Art History Refl.pdf}
}

@misc{Gallica,
  title = {Gallica},
  url = {https://gallica.bnf.fr/},
  urldate = {2023-08-10},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/G684L5BJ/accueil-fr.html}
}

@misc{GeoGebra,
  title = {{GeoGebra}},
  journal = {GeoGebra},
  url = {https://www.geogebra.org/},
  urldate = {2023-09-01},
  abstract = {R\'esoudre des \'equations, repr\'esenter des fonctions, cr\'eer des constructions, analyser des donn\'ees, explorer la 3D !},
  langid = {french},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/7FU53J2C/www.geogebra.org.html}
}

@misc{Gephi,
  title = {Gephi},
  url = {https://gephi.org/},
  urldate = {2023-09-01},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/VQE9JJ5V/gephi.org.html}
}

@book{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}},
  url = {http://www.deeplearningbook.org},
  keywords = {ml}
}

@misc{GoogleArtsCulture,
  title = {{Google Arts \& Culture}},
  url = {https://artsandculture.google.com/},
  urldate = {2023-08-10},
  abstract = {Google Arts~\& Culture pr\'esente le contenu de plus de 2~000~grands mus\'ees et centres d'archives qui ont collabor\'e avec l'Institut culturel de Google afin de mettre les tr\'esors du monde en ligne.},
  langid = {french},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/IR4PP5H6/artsandculture.google.com.html}
}

@misc{grinbergRESTfulAuthenticationFlask,
  title = {{{RESTful Authentication}} with {{Flask}}},
  author = {Grinberg, Miguel},
  url = {http://blog.miguelgrinberg.com/post/restful-authentication-with-flask},
  urldate = {2023-08-27},
  abstract = {This article is the fourth in my series on RESTful APIs. Today I will be showing you a simple, yet secure way to protect a Flask based API with password or token based authentication.This article\ldots},
  langid = {english},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/4CSIVACK/restful-authentication-with-flask.html}
}

@misc{GuidePratiquePour,
  title = {Guide Pratique Pour La Recherche et La R\'eutilisation Des Images d'\oe uvres d'art},
  publisher = {{Institut nationale d'histoire de l'art}},
  url = {https://www.inha.fr/_resources/PDF/2021/2021_LIVRET_GUIDE-PRATIQUE_web_15062021.pdf},
  keywords = {technique}
}

@book{hamburgerDiagramParadigmCrossCultural2022,
  title = {The {{Diagram}} as {{Paradigm}}: {{Cross-Cultural Approaches}}},
  shorttitle = {The {{Diagram}} as {{Paradigm}}},
  author = {Hamburger, Jeffrey F. and Roxburgh, David J. and Safran, Linda},
  year = {2022},
  series = {Dumbarton {{Oaks Byzantine Symposia}} and {{Colloquia}}},
  publisher = {{Harvard University Press}},
  address = {{Cambridge, MA}},
  abstract = {The Diagram as Paradigm is the first book that looks at medieval diagrams in a cross-cultural perspective, focusing on three regions\textemdash Byzantium, the Islamicate world, and the Latin West\textemdash each culturally diverse and each closely linked to the others through complex processes of intellectual, artistic, diplomatic, and mercantile exchange. The volume unites case studies, often of little-known material, by an international set of specialists, and is prefaced by four introductory essays that provide broad overviews of diagrammatic traditions in these regions in addition to considering the theoretical dimensions of diagramming. Among the historical disciplines whose use of diagrams is explored are philosophy, theology, mysticism, music, medicine, mathematics, astronomy, and cosmology. Despite the sheer variety, ingenuity, and visual inventiveness of diagrams from the premodern world, in conception and practical use they often share many similarities, both in construction and application. Diagrams prove to be an essential part of the fabric of premodern intellectual, scientific, religious, artistic, and artisanal life.},
  isbn = {978-0-88402-486-6},
  keywords = {astronomie},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/JNYJ9NV4/catalog.html}
}

@misc{HistoricalNavigationDISHAS,
  title = {Historical Navigation \textemdash{} {{DISHAS}} Project \textemdash{} {{Astronomical}} Table in Database},
  url = {https://dishas.obspm.fr/historical-navigation},
  urldate = {2023-08-10},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/ZU79XDST/historical-navigation.html}
}

@misc{HomeDISHASProject,
  title = {Home \textemdash{} {{DISHAS}} Project \textemdash{} {{Astronomical}} Table in Database},
  url = {https://dishas.obspm.fr/},
  urldate = {2023-08-10},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/EUMAF34Z/dishas.obspm.fr.html}
}

@misc{HowAuthenticateFlask,
  title = {How {{To Authenticate Flask API Using JWT}}},
  journal = {LoginRadius Blog},
  url = {https://www.loginradius.com/blog/engineering/guest-post/securing-flask-api-with-jwt/},
  urldate = {2023-08-27},
  abstract = {This tutorial helps you build a simple Flask API and demonstrates how to secure it using JWT. In the end, you can test your API authentication using a sample schema.},
  langid = {english},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/9EBZWZFM/securing-flask-api-with-jwt.html}
}

@misc{HowItWorks,
  title = {How {{It Works}}},
  url = {https://iiif.io/get-started/how-iiif-works/},
  urldate = {2023-08-08},
  abstract = {IIIF is a set of open standards for delivering high-quality, attributed digital objects online at scale. It's also an international community developing and implementing the IIIF APIs. IIIF is backed by a consortium of leading cultural institutions.},
  langid = {english},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/DAB6RRVW/how-iiif-works.html}
}

@misc{IIIFMuseumsFrance2023,
  title = {{{IIIF}} for Museums in {{France}}},
  year = {2023},
  month = jul,
  url = {https://www.culture.gouv.fr/en/Thematic/Museums/Pour-les-professionnels/Network/IIIF-for-museums-in-France},
  urldate = {2023-08-09},
  abstract = {IIIF (International Image Interoperability Framework) is a community and common technical framework for the dissemination and manipulation of digital images on the Web. Museums in France, major producers and collectors of images and heritage data, have every interest in exploring the many functional},
  langid = {british},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/IM4LL8GV/IIIF-for-museums-in-France.html}
}

@misc{IIIFViewers,
  title = {{{IIIF Viewers}}},
  url = {https://iiif.io/get-started/iiif-viewers/},
  urldate = {2023-08-08},
  abstract = {IIIF is a set of open standards for delivering high-quality, attributed digital objects online at scale. It's also an international community developing and implementing the IIIF APIs. IIIF is backed by a consortium of leading cultural institutions.},
  langid = {english},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/LNTVIBR5/iiif-viewers.html}
}

@misc{ImageAPI,
  title = {Image {{API}} 3.0},
  url = {https://iiif.io/api/image/3.0/},
  urldate = {2023-08-08},
  abstract = {IIIF is a set of open standards for delivering high-quality digital objects online at scale. It's also the international community that makes it all work.},
  langid = {english},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/HPPKYNR3/2.1.html}
}

@misc{ImageNet,
  title = {{{ImageNet}}},
  url = {https://www.image-net.org/},
  urldate = {2023-08-10},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/X7TA2JSH/www.image-net.org.html}
}

@misc{Inkscape,
  title = {{Inkscape}},
  url = {https://inkscape.org/fr/},
  urldate = {2023-09-01},
  abstract = {Inkscape est un logiciel professionnel de dessin vectoriel pour Windows, Mac OS X et GNU/Linux. Il est libre et gratuit.},
  langid = {french},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/VKCPHBK4/fr.html}
}

@misc{jacquotDecrireTranscrireDiffuser2017,
  type = {{Billet}},
  title = {{D\'ecrire, transcrire et diffuser un corpus documentaire h\'et\'erog\`ene : m\'ethodes, formats, outils}},
  shorttitle = {{D\'ecrire, transcrire et diffuser un corpus documentaire h\'et\'erog\`ene}},
  author = {Jacquot, Olivier},
  year = {2017},
  month = nov,
  journal = {Carnet de la recherche \`a la Biblioth\`eque nationale de France},
  url = {https://bnf.hypotheses.org/2214},
  urldate = {2023-08-06},
  abstract = {Dans le cadre du projet CORPUS, inscrit au plan quadriennal de la recherche 2016-2019 de la Biblioth\`eque nationale de France, un atelier ~intitul\'e \guillemotleft ~D\'ecrire, transcrire et diffuser un corpus documentaire h\'et\'erog\`ene : m\'ethodes, formats, outils~\guillemotright...},
  langid = {french},
  keywords = {DH},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/SBUUZXXG/2214.html}
}

@article{jardineCriticalEditingEarlyModern2010,
  title = {Critical {{Editing}} of {{Early-Modern Astronomical Diagrams}}},
  author = {Jardine, Boris and Jardine, Nicholas},
  year = {2010},
  month = aug,
  journal = {Journal for the History of Astronomy},
  volume = {41},
  number = {3},
  pages = {393--414},
  publisher = {{SAGE Publications Ltd}},
  issn = {0021-8286},
  doi = {10.1177/002182861004100307},
  url = {https://doi.org/10.1177/002182861004100307},
  urldate = {2023-08-01},
  langid = {english},
  keywords = {astronomie}
}

@misc{jocherYOLOv5Ultralytics2020,
  title = {{{YOLOv5}} by {{Ultralytics}}},
  author = {Jocher, Glenn},
  year = {2020},
  month = may,
  doi = {10.5281/zenodo.3908559},
  url = {https://github.com/ultralytics/yolov5},
  urldate = {2023-08-28},
  abstract = {YOLOv5 🚀 in PyTorch {$>$} ONNX {$>$} CoreML {$>$} TFLite},
  copyright = {AGPL-3.0}
}

@incollection{jonesPtolemya,
  title = {Ptolemy},
  booktitle = {Encyclopedia {{Britannica}}},
  author = {Jones, Alexander Raymond},
  url = {https://www.britannica.com/biography/Ptolemy},
  urldate = {2023-08-03},
  keywords = {astronomie}
}

@article{joyeux-prunelBasesDonneesGestion2018,
  title = {{Bases de donn\'ees et gestion de projets en humanit\'es num\'eriques}},
  author = {{Joyeux-Prunel}, B{\'e}atrice},
  year = {2018},
  month = feb,
  journal = {Biens Symboliques / Symbolic Goods. Revue de sciences sociales sur les arts, la culture et les id\'ees},
  number = {2},
  publisher = {{Presses Universitaires de Vincennces}},
  issn = {2490-9424},
  doi = {10.4000/bssg.242},
  url = {https://journals.openedition.org/bssg/242?lang=en},
  urldate = {2023-08-06},
  abstract = {1. Comment est n\'e le projet de base de donn\'ees~? Le projet ARTL@S met \`a disposition des chercheur{$\cdot$}se{$\cdot$}s une base num\'erique de catalogues d'expositions partout dans le monde aux xixe et xxe si\`ecles. Les adresses contenues dans ces catalogues sont g\'eor\'ef\'erenc\'ees, quelles que soient les \'epoques et les g\'eographies. Cet outil est pens\'e pour permettre aussi bien des recherches monographiques qu'un travail sur les circulations des \oe uvres d'art, la construction des carri\`eres et des go\^uts dans ce proce...},
  copyright = {All rights reserved},
  langid = {french},
  keywords = {DH},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/R537EZYH/Joyeux-Prunel - 2018 - Bases de données et gestion de projets en humanité.pdf}
}

@article{joyeux-prunelCeQueApproche2014,
  title = {{Ce que l'approche mondiale fait \`a l'histoire de l'art}},
  author = {{Joyeux-Prunel}, B{\'e}atrice},
  year = {2014},
  journal = {Romantisme},
  volume = {163},
  number = {1},
  pages = {63--78},
  publisher = {{Armand Colin}},
  address = {{Paris}},
  issn = {0048-8593},
  doi = {10.3917/rom.163.0063},
  url = {https://www.cairn.info/revue-romantisme-2014-1-page-63.htm},
  urldate = {2023-08-06},
  abstract = {Les arts sont l'un des chantiers les plus importants pour le projet d'une histoire mondiale. Deux types d'approches dominent~: l'une, inspir\'ee par les probl\'ematiques postcoloniales~; l'autre, par le projet d'une histoire des circulations culturelles. Les historiens de l'art au XIXe si\`ecle ont en g\'en\'eral opt\'e pour une relecture transnationale de l'histoire de l'art, focalis\'ee sur l'\'etude des circulations artistiques et leurs effets. Cet article pr\'esente les principaux champs ouverts \`a la recherche sur la mondialisation artistique~: \'etude internationale du march\'e, reconstitution des conditions techniques et juridiques de circulation des \oe uvres, interrogations sur les m\'etissages et la g\'en\'etique transculturelle des \oe uvres et des go\^uts. On s'interroge enfin sur ce que l'approche mondiale fait \`a l'histoire de l'art comme discipline, et les renouvellements qu'elle appelle.},
  langid = {french},
  keywords = {DH},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/KWGSG2L9/revue-romantisme-2014-1-page-63.html}
}

@article{klinkeBigImageData2016,
  title = {Big {{Image Data}} within the {{Big Picture}} of {{Art History}}},
  author = {Klinke, Harald},
  year = {2016},
  month = oct,
  journal = {International Journal for Digital Art History},
  number = {2},
  issn = {2363-5401},
  doi = {10.11588/dah.2016.2.33527},
  url = {https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/33527},
  urldate = {2023-08-01},
  abstract = {The use of the computer in Art History is changing the approach towards our objects of research. Now, we are able to compute more images than a human can see in a lifetime. That, in turn, calls for a new definition of the role of the researcher and the tools being used. The access to large amounts of visual data stands in a tradition of conventional methods of Art History, but also augments them with quantity. This article proposes a theoretical model on which to build an understanding of the meta image with which we interactively derive our conclusions.},
  copyright = {Copyright (c) 2016 International Journal for Digital Art History},
  langid = {english},
  keywords = {DH},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/RE3A5PGA/Klinke - 2016 - Big Image Data within the Big Picture of Art Histo.pdf}
}

@misc{LeibnizOnlineLeibnizEditionc,
  title = {Leibniz {{Online}} - {{Leibniz-Edition Berlin}}},
  url = {https://leibniz-berlin.bbaw.de/de/leibniz-online},
  urldate = {2023-09-01},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/D4ZUYBUE/leibniz-online.html}
}

@misc{linComprehensiveReviewImage2023,
  title = {A {{Comprehensive Review}} of {{Image Line Segment Detection}} and {{Description}}: {{Taxonomies}}, {{Comparisons}}, and {{Challenges}}},
  shorttitle = {A {{Comprehensive Review}} of {{Image Line Segment Detection}} and {{Description}}},
  author = {Lin, Xinyu and Zhou, Yingjie and Liu, Yipeng and Zhu, Ce},
  year = {2023},
  month = apr,
  number = {arXiv:2305.00264},
  eprint = {2305.00264},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.00264},
  url = {http://arxiv.org/abs/2305.00264},
  urldate = {2023-08-10},
  abstract = {Detection and description of line segments lay the basis for numerous vision tasks. Although many studies have aimed to detect and describe line segments, a comprehensive review is lacking, obstructing their progress. This study fills the gap by comprehensively reviewing related studies on detecting and describing two-dimensional image line segments to provide researchers with an overall picture and deep understanding. Based on their mechanisms, two taxonomies for line segment detection and description are presented to introduce, analyze, and summarize these studies, facilitating researchers to learn about them quickly and extensively. The key issues, core ideas, advantages and disadvantages of existing methods, and their potential applications for each category are analyzed and summarized, including previously unknown findings. The challenges in existing methods and corresponding insights for potentially solving them are also provided to inspire researchers. In addition, some state-of-the-art line segment detection and description algorithms are evaluated without bias, and the evaluation code will be publicly available. The theoretical analysis, coupled with the experimental results, can guide researchers in selecting the best method for their intended vision applications. Finally, this study provides insights for potentially interesting future research directions to attract more attention from researchers to this field.},
  archiveprefix = {arxiv},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/LSQMA4DK/Lin et al. - 2023 - A Comprehensive Review of Image Line Segment Detec.pdf;/home/jade/snap/zotero-snap/common/Zotero/storage/SMBYA2LF/2305.html}
}

@misc{malloryIIIFMuseumsExplained2019,
  title = {{{IIIF}} for Museums, Explained},
  author = {Mallory, Gavin},
  year = {2019},
  month = jul,
  journal = {Medium},
  url = {https://blog.cogapp.com/iiif-for-museums-explained-49fd0560e1ba},
  urldate = {2023-08-09},
  abstract = {What IIIF is; who's using it; and why you should consider it},
  langid = {english},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/KAEQVKZM/iiif-for-museums-explained-49fd0560e1ba.html}
}

@article{mancaNouveauxDefisAgences2018,
  title = {{Les nouveaux d\'efis des agences photographiques des mus\'ees}},
  author = {Manca, Isabelle},
  year = {2018},
  month = jun,
  journal = {Le Journal Des Arts},
  url = {https://www.lejournaldesarts.fr/patrimoine/les-nouveaux-defis-des-agences-photographiques-des-musees-138076},
  urldate = {2023-08-06},
  abstract = {Contrairement \`a une id\'ee re\c{c}ue, les mus\'ees fran\c{c}ais ne sont pas plus frileux que la tr\`es grande majorit\'e de leurs confr\`eres \`a l'\'etranger pour diffuser largement sur Internet leurs collections. Ils ne veulent pas pour autant se priver de recettes.},
  langid = {french},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/RMBUITBR/les-nouveaux-defis-des-agences-photographiques-des-musees-138076.html}
}

@misc{MiradorHomea,
  title = {Mirador \textemdash{} {{Home}}},
  url = {https://projectmirador.org/},
  urldate = {2023-09-01},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/LLVT8648/projectmirador.org.html}
}

@misc{moiraghiExplorerCorpusImages2018,
  type = {{Billet}},
  title = {{Explorer des corpus d'images. L'IA au service du patrimoine}},
  author = {Moiraghi, Eleonora and Moreux, Jean-Philippe},
  year = {2018},
  month = apr,
  journal = {Carnet de la recherche \`a la Biblioth\`eque nationale de France},
  url = {https://bnf.hypotheses.org/2809},
  urldate = {2023-08-06},
  abstract = {Suite aux ateliers \guillemotleft{} D\'ecrire, transcrire et diffuser un corpus documentaire h\'et\'erog\`ene : m\'ethodes, formats, outils \guillemotright{} et \guillemotleft ~G\'eolocalisation et spatialisation de documents patrimoniaux~\guillemotright, une troisi\`eme demi-journ\'ee d'\'etude a \'et\'e organis\'ee dans le cadre du...},
  langid = {french},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/EA5XSUVW/2809.html}
}

@misc{monnierDocExtractor2023,
  title = {{{docExtractor}}},
  author = {Monnier, Tom},
  year = {2023},
  month = jul,
  url = {https://github.com/monniert/docExtractor},
  urldate = {2023-08-10},
  abstract = {(ICFHR 2020 oral) Code for "docExtractor: An off-the-shelf historical document element extraction" paper},
  copyright = {MIT}
}

@inproceedings{monnierDocExtractorOfftheshelfHistorical2020,
  title = {{{docExtractor}}: {{An}} off-the-Shelf Historical Document Element Extraction},
  shorttitle = {{{docExtractor}}},
  booktitle = {2020 17th {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}} ({{ICFHR}})},
  author = {Monnier, Tom and Aubry, Mathieu},
  year = {2020},
  month = sep,
  eprint = {2012.08191},
  primaryclass = {cs},
  pages = {91--96},
  doi = {10.1109/ICFHR2020.2020.00027},
  url = {http://arxiv.org/abs/2012.08191},
  urldate = {2023-08-14},
  abstract = {We present docExtractor, a generic approach for extracting visual elements such as text lines or illustrations from historical documents without requiring any real data annotation. We demonstrate it provides high-quality performances as an off-the-shelf system across a wide variety of datasets and leads to results on par with state-of-the-art when fine-tuned. We argue that the performance obtained without fine-tuning on a specific dataset is critical for applications, in particular in digital humanities, and that the line-level page segmentation we address is the most relevant for a general purpose element extraction engine. We rely on a fast generator of rich synthetic documents and design a fully convolutional network, which we show to generalize better than a detection-based approach. Furthermore, we introduce a new public dataset dubbed IlluHisDoc dedicated to the fine evaluation of illustration segmentation in historical documents.},
  archiveprefix = {arxiv},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/G96IABYM/Monnier et Aubry - 2020 - docExtractor An off-the-shelf historical document.pdf;/home/jade/snap/zotero-snap/common/Zotero/storage/YKYGZPB4/2012.html}
}

@misc{monnierUnsupervisedLayeredImage2021,
  title = {Unsupervised {{Layered Image Decomposition}} into {{Object Prototypes}}},
  author = {Monnier, Tom and Vincent, Elliot and Ponce, Jean and Aubry, Mathieu},
  year = {2021},
  month = aug,
  number = {arXiv:2104.14575},
  eprint = {2104.14575},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.14575},
  url = {http://arxiv.org/abs/2104.14575},
  urldate = {2023-08-05},
  abstract = {We present an unsupervised learning framework for decomposing images into layers of automatically discovered object models. Contrary to recent approaches that model image layers with autoencoder networks, we represent them as explicit transformations of a small set of prototypical images. Our model has three main components: (i) a set of object prototypes in the form of learnable images with a transparency channel, which we refer to as sprites; (ii) differentiable parametric functions predicting occlusions and transformation parameters necessary to instantiate the sprites in a given image; (iii) a layered image formation model with occlusion for compositing these instances into complete images including background. By jointly learning the sprites and occlusion/transformation predictors to reconstruct images, our approach not only yields accurate layered image decompositions, but also identifies object categories and instance parameters. We first validate our approach by providing results on par with the state of the art on standard multi-object synthetic benchmarks (Tetrominoes, Multi-dSprites, CLEVR6). We then demonstrate the applicability of our model to real images in tasks that include clustering (SVHN, GTSRB), cosegmentation (Weizmann Horse) and object discovery from unfiltered social network images. To the best of our knowledge, our approach is the first layered image decomposition algorithm that learns an explicit and shared concept of object type, and is robust enough to be applied to real images.},
  archiveprefix = {arxiv},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/FFZTA77T/Monnier et al. - 2021 - Unsupervised Layered Image Decomposition into Obje.pdf;/home/jade/snap/zotero-snap/common/Zotero/storage/SM5I5PZE/2104.html}
}

@article{musikComputerVisionDigital2018,
  title = {Computer {{Vision}} and the {{Digital Humanities}}: {{Adapting Image Processing Algorithms}} and {{Ground Truth}} through {{Active Learning}}},
  shorttitle = {Computer {{Vision}} and the {{Digital Humanities}}},
  author = {Musik, Christoph and Zeppelzauer, Matthias},
  year = {2018},
  month = dec,
  journal = {VIEW Journal of European Television History and Culture},
  volume = {7},
  number = {14},
  pages = {59},
  issn = {2213-0969},
  doi = {10.18146/2213-0969.2018.jethc153},
  url = {https://www.viewjournal.eu/article/10.18146/2213-0969.2018.jethc153/},
  urldate = {2023-08-10},
  abstract = {Automated computer vision methods and tools offer new ways of analysing audio-visual material in the realm of the Digital Humanities (DH). While there are some promising results where these tools can be applied, there are basic challenges, such as algorithmic bias and the lack of sufficient transparency, one needs to carefully use these tools in a productive and responsible way. When it comes to the socio-technical understanding of computer vision tools and methods, a major unit of sociological analysis, attentiveness, and access for configuration (for both computer vision scientists and DH scholars) is what computer science calls ``ground truth''. What is specified in the ground truth is the template or rule to follow, e.g. what an object looks like. This article aims at providing scholars in the DH with knowledge about how automated tools for image analysis work and how they are constructed. Based on these insights, the paper introduces an approach called ``active learning'' that can help to configure these tools in ways that fit the specific requirements and research questions of the DH in a more adaptive and user-centered way. We argue that both objectives need to be addressed, as this is, by all means, necessary for a successful implementation of computer vision tools in the DH and related fields.},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/HQ4BHCDS/Musik et Zeppelzauer - 2018 - Computer Vision and the Digital Humanities Adapti.pdf}
}

@misc{norindrElementExtractionGPU2023,
  title = {Element Extraction on a {{GPU}}},
  author = {Norindr, Jade},
  year = {2023},
  month = jun,
  url = {https://github.com/jnorindr/extractorAPI},
  urldate = {2023-08-26},
  abstract = {API to run inference on a GPU using a vision model for element extraction}
}

@misc{NumerisationBnF,
  title = {{La num\'erisation \`a la BnF}},
  journal = {BnF},
  url = {https://www.bnf.fr/fr/la-numerisation-la-bnf},
  urldate = {2023-08-06},
  langid = {french},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/UA9GEV4R/la-numerisation-la-bnf.html}
}

@misc{pollockComputersCanFind2014,
  title = {Computers Can Find Similarities between Paintings \textendash{} but Art History Is about so Much More},
  author = {Pollock, Griselda},
  year = {2014},
  month = aug,
  journal = {The Conversation},
  url = {http://theconversation.com/computers-can-find-similarities-between-paintings-but-art-history-is-about-so-much-more-30752},
  urldate = {2023-08-01},
  abstract = {Some computer scientists at Rutgers University in New Jersey have written a computer programme that finds connections between paintings and can even discover influences between artists, they claim. This\ldots},
  langid = {english},
  keywords = {DH}
}

@misc{PresentationAPI,
  title = {Presentation {{API}} 3.0},
  url = {https://iiif.io/api/presentation/3.0/},
  urldate = {2023-08-08},
  abstract = {IIIF is a set of open standards for delivering high-quality digital objects online at scale. It's also the international community that makes it all work.},
  langid = {english},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/F7D6NF7K/3.0.html}
}

@misc{PyTorch,
  title = {{{PyTorch}}},
  url = {https://pytorch.org/},
  urldate = {2023-08-18},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/JFLRAS67/pytorch.org.html}
}

@article{raynaudBuildingStemmaCodicum2014,
  title = {Building the Stemma Codicum from Geometric Diagrams: {{A}} Treatise on Optics by {{Ibn}} al-{{Haytham}} as a Test Case},
  shorttitle = {Building the Stemma Codicum from Geometric Diagrams},
  author = {Raynaud, Dominique},
  year = {2014},
  journal = {Archive for History of Exact Sciences},
  volume = {68},
  number = {2},
  eprint = {24569630},
  eprinttype = {jstor},
  pages = {207--239},
  publisher = {{Springer}},
  issn = {0003-9519},
  url = {https://www.jstor.org/stable/24569630},
  urldate = {2023-08-01},
  abstract = {In view of the progress made in recent decades in the fields of stemmatology and the analysis of geometric diagrams, the present article explores the possibility of establishing the stemma codicum of a handwritten tradition from geometric diagrams alone. This exploratory method is tested on Ibn al-Haytham's Epistle on the Shape of the Eclipse, because this work has not yet been issued in a critical edition. Separate stemmata were constructed on the basis of the diagrams and the text, and a comparison showed no major differences. The greater reliability of a stemma codicum constructed on the basis of the diagrams rather than the text of a mathematical work is discussed, and preliminary conclusions are drawn.},
  keywords = {astronomie}
}

@article{richardProgrammeNumerisationBibliotheque1993,
  title = {{Le programme de num\'erisation de la Biblioth\`eque de France}},
  author = {Richard, Michel},
  year = {1993},
  journal = {Bulletin des biblioth\`eques de France},
  number = {3},
  pages = {53--63},
  url = {https://bbf.enssib.fr/consulter/bbf-1993-03-0053-007},
  urldate = {2023-08-06},
  abstract = {L'\'etablissement public charg\'e de la construction de la Biblioth\`eque de France a l'ambition de constituer d'importantes collections sur support num\'erique : textes, musique, images fixes. Il d\'eveloppe pour la consultation une station de travail, le poste de lecture assist\'ee par ordinateur (PLAO), permettant la recherche approfondie sur des textes structur\'es. Les documents num\'eris\'es seront transmissibles \`a distance au sein du r\'eseau des biblioth\`eques associ\'ees. Le coeur de la collection sera constitu\'e des oeuvres des auteurs classiques fran\c{c}ais et des ouvrages de r\'ef\'erence des sciences humaines et de l'histoire des sciences. Le caract\`ere exp\'erimental de ce programme a permis d'y associer plusieurs laboratoires de recherche.},
  langid = {french},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/B5IR92ND/bbf-1993-03-0053-007.html}
}

@misc{ronseInvarianceParTranslations,
  title = {Invariance Par Translations},
  author = {Ronse, Christian},
  url = {https://dpt-info.u-strasbg.fr/~cronse/TIDOC/SYM/invtr.html},
  urldate = {2023-08-15},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/CL6MWKDA/invtr.html}
}

@misc{rousseauEpicyclesPtolemee,
  title = {Epicycles de {{Ptol\'em\'ee}}},
  author = {Rousseau, Jean-Jacques},
  url = {http://ressources.univ-lemans.fr/AccesLibre/UM/Pedago/physique/02/divers/ptolemee.html},
  urldate = {2023-08-05},
  keywords = {astronomie},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/EQMDGAMU/ptolemee.html}
}

@misc{sajjanshettyDeployingPyTorchPython,
  title = {Deploying {{PyTorch}} in {{Python}} via a {{REST API}} with {{Flask}} \textemdash{} {{PyTorch Tutorials}} 2.0.1+cu117 Documentation},
  author = {Sajjanshetty, Avinash},
  url = {https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html},
  urldate = {2023-08-27},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/ITDA5NDB/flask_rest_api_tutorial.html}
}

@misc{sakaiDigitizingDisparityMuseum2021,
  title = {Digitizing Disparity in the Museum. {{The}} Object-Based Research in the {{Tokyo National Museum}}},
  author = {Sakai, Akira},
  year = {2021},
  month = aug,
  journal = {University Heritage},
  url = {https://universityheritage.eu/en/digitizing-disparity-in-the-museum-the-object-based-research-in-the-tokyo-national-museum/},
  urldate = {2023-08-06},
  abstract = {In May 2020, UNESCO and ICOM published two reports of studies about how the COVID-19 outbreak is affecting museums around the world. While UNESCO (2020, p. 6) highlights the serious digital divide between museums among regions, ICOM (2020, pp. 9-13) reported that many museums enhanced their digital activities during the lockdown. According to both reports, [\ldots ]},
  langid = {british},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/IDRCS5EN/digitizing-disparity-in-the-museum-the-object-based-research-in-the-tokyo-national-museum.html}
}

@misc{sandyChoosingDjangoFlask,
  title = {Choosing between {{Django}}, {{Flask}}, and {{FastAPI}}},
  author = {Sandy, James},
  journal = {Engineering Education (EngEd) Program | Section},
  url = {https://www.section.io/engineering-education/choosing-between-django-flask-and-fastapi/},
  urldate = {2023-08-27},
  abstract = {This article will highlight some features that will enable a beginner to make an informed decision between Django, Flask, or Fast API.},
  langid = {american},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/5288VXUH/choosing-between-django-flask-and-fastapi.html}
}

@incollection{sepetjanRespecterDroitPropriete2017,
  title = {{Respecter le droit de la propri\'et\'e litt\'eraire et artistique}},
  booktitle = {{Num\'eriser et mettre en ligne}},
  author = {Sepetjan, Sophie},
  editor = {Claerr, Thierry and Westeel, Isabelle},
  year = {2017},
  series = {{La Bo\^ite \`a outils}},
  pages = {46--64},
  publisher = {{Presses de l'enssib}},
  address = {{Villeurbanne}},
  doi = {10.4000/books.pressesenssib.426},
  url = {http://books.openedition.org/pressesenssib/426},
  urldate = {2023-08-06},
  copyright = {https://www.openedition.org/12554},
  isbn = {978-2-37546-032-0},
  langid = {french},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/QHA68L7H/Sepetjan - 2017 - Partie II. Respecter le droit de la propriété litt.pdf}
}

@misc{sharabokWhyDeepLearning2020,
  title = {Why {{Deep Learning Uses GPUs}}?},
  author = {Sharabok, German},
  year = {2020},
  month = jul,
  journal = {Medium},
  url = {https://towardsdatascience.com/why-deep-learning-uses-gpus-c61b399e93a0},
  urldate = {2023-08-26},
  abstract = {And why you should too\ldots},
  langid = {english},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/9SBYB4UW/why-deep-learning-uses-gpus-c61b399e93a0.html}
}

@misc{sharmaTrainingYOLOv5Object2022,
  title = {Training the {{YOLOv5 Object Detector}} on a {{Custom Dataset}}},
  author = {Sharma, Aditya},
  year = {2022},
  month = jun,
  journal = {PyImageSearch},
  url = {https://pyimagesearch.com/2022/06/20/training-the-yolov5-object-detector-on-a-custom-dataset/},
  urldate = {2023-08-18},
  abstract = {Learn to train a YOLOv5 object detector on a custom dataset in the PyTorch framework.},
  langid = {american},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/P9P2B3B5/training-the-yolov5-object-detector-on-a-custom-dataset.html}
}

@misc{shenLargeScaleHistoricalWatermark2019,
  title = {Large-{{Scale Historical Watermark Recognition}}: Dataset and a New Consistency-Based Approach},
  shorttitle = {Large-{{Scale Historical Watermark Recognition}}},
  author = {Shen, Xi and Pastrolin, Ilaria and Bounou, Oumayma and Gidaris, Spyros and Smith, Marc and Poncet, Olivier and Aubry, Mathieu},
  year = {2019},
  month = aug,
  number = {arXiv:1908.10254},
  eprint = {1908.10254},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1908.10254},
  url = {http://arxiv.org/abs/1908.10254},
  urldate = {2023-08-25},
  abstract = {Historical watermark recognition is a highly practical, yet unsolved challenge for archivists and historians. With a large number of well-defined classes, cluttered and noisy samples, different types of representations, both subtle differences between classes and high intra-class variation, historical watermarks are also challenging for pattern recognition. In this paper, overcoming the difficulty of data collection, we present a large public dataset with more than 6k new photographs, allowing for the first time to tackle at scale the scenarios of practical interest for scholars: one-shot instance recognition and cross-domain one-shot instance recognition amongst more than 16k fine-grained classes. We demonstrate that this new dataset is large enough to train modern deep learning approaches, and show that standard methods can be improved considerably by using mid-level deep features. More precisely, we design both a matching score and a feature fine-tuning strategy based on filtering local matches using spatial consistency. This consistency-based approach provides important performance boost compared to strong baselines. Our model achieves 55\% top-1 accuracy on our very challenging 16,753-class one-shot cross-domain recognition task, each class described by a single drawing from the classic Briquet catalog. In addition to watermark classification, we show our approach provides promising results on fine-grained sketch-based image retrieval.},
  archiveprefix = {arxiv},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/HXL6M9PN/Shen et al. - 2019 - Large-Scale Historical Watermark Recognition data.pdf;/home/jade/snap/zotero-snap/common/Zotero/storage/2SJG96UQ/1908.html}
}

@article{strienComputerVisionHumanities2022,
  title = {Computer {{Vision}} for the {{Humanities}}: {{An Introduction}} to {{Deep Learning}} for {{Image Classification}} ({{Part}} 1)},
  shorttitle = {Computer {{Vision}} for the {{Humanities}}},
  author = {van Strien, Daniel and Beelen, Kaspar and Wevers, Melvin and Smits, Thomas and McDonough, Katherine},
  year = {2022},
  month = aug,
  journal = {Programming Historian},
  url = {https://programminghistorian.org/en/lessons/computer-vision-deep-learning-pt1},
  urldate = {2023-08-10},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/T7JUD62Y/computer-vision-deep-learning-pt1.html}
}

@misc{TelotaLeibnizVIIILaTeX_TEIa,
  title = {Telota/{{LeibnizVIII-LaTeX}}\_{{TEI}}},
  url = {https://github.com/telota/LeibnizVIII-LaTeX_TEI/tree/main},
  urldate = {2023-09-01},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/7ET932DK/main.html}
}

@misc{ultralyticsUltralyticsYOLOv8Docs,
  title = {Ultralytics {{YOLOv8 Docs}}},
  author = {Ultralytics},
  url = {https://docs.ultralytics.com/},
  urldate = {2023-08-15},
  abstract = {Explore a complete guide to Ultralytics YOLOv8, a high-speed, high-accuracy object detection \& image segmentation model. Installation, prediction, training tutorials and more.},
  langid = {english},
  keywords = {technique},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/QR5ZIXR2/docs.ultralytics.com.html}
}

@misc{UsingComputerVision,
  title = {Using Computer Vision Tools for Historical Newspaper Analysis: {{SIAMESE}} and {{Europeana Newspapers}}},
  shorttitle = {Using Computer Vision Tools for Historical Newspaper Analysis},
  journal = {Europeana PRO},
  url = {https://pro.europeana.eu/post/using-computer-vision-tools-for-historical-newspaper-analysis-siamese-and-europeana-newspapers},
  urldate = {2023-08-01},
  abstract = {An interview with Melvin Wevers, Digital Humanities researcher at KNAW, and Clemens Neudecker, coordinator of Europeana Newspapers},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/3U2T74KU/using-computer-vision-tools-for-historical-newspaper-analysis-siamese-and-europeana-newspapers.html}
}

@misc{vallerianiDetectingVisualElements2022,
  title = {Detecting {{Visual Elements}} in {{Historical Documents}}},
  author = {Valleriani, Matteo and B{\"u}ttner, Jochen and {El-Hajj}, Hassan and Martinetz, Julius},
  year = {2022},
  month = dec,
  url = {https://www.bifold.berlin/news-events/news/view/news-detail/detecting-visual-elements-in-historical-documents},
  urldate = {2023-08-10},
  abstract = {Historians are increasingly in need of digital tools to process and extract information from electronic copies of historical sources. BIFOLD scientists developed\&nbsp;YOLO (You Only Look Once).},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/X4KZQNFV/detecting-visual-elements-in-historical-documents.html}
}

@misc{WelcomeSimpleAnnotation,
  title = {Welcome to the {{Simple Annotation Server}}},
  url = {https://dev.gdmrdigital.com/},
  urldate = {2023-08-29},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/255B825I/dev.gdmrdigital.com.html}
}
