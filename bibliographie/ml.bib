@misc{arslanCreateMachineLearning2021,
  title = {Create a {{Machine Learning API With Django Rest Framework}} | {{Better Programming}}},
  author = {Arslan, G{\"o}rkem},
  year = {2021},
  month = jul,
  url = {https://betterprogramming.pub/create-a-machine-learning-api-with-django-rest-framework-967571640c46},
  urldate = {2023-08-27},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/TCB9CHD3/create-a-machine-learning-api-with-django-rest-framework-967571640c46.html}
}

@book{azencottIntroductionAuMachine2018,
  title = {{Introduction au Machine Learning}},
  author = {Azencott, Chlo{\'e}-Agathe},
  year = {2018},
  month = sep,
  publisher = {{Dunod}},
  address = {{Malakoff}},
  abstract = {Cet ouvrage s'adresse aux \'etudiants en fin de licence et en master d'informatique ou de maths appliqu\'ees, ainsi qu'aux \'el\`eves ing\'enieurs. Le Machine Learning est une discipline dont les outils puissants permettent aujourd'hui \`a de nombreux secteurs d'activit\'e de r\'ealiser des progr\`es spectaculaires gr\^ace \`a l'exploitation de grands volumes de donn\'ees. Le but de cet ouvrage est de vous fournir des bases solides sur les concepts et les algorithmes de ce domaine en plein essor. Il vous aidera \`a identifier les probl\`emes qui peuvent \^etre r\'esolus par une approche Machine Learning, \`a les formaliser, \`a identifier les algorithmes les mieux adapt\'es \`a chaque probl\`eme, \`a les mettre en oeuvre, et enfin \`a savoir \'evaluer les r\'esultats obtenus. Les notions de cours sont illustr\'ees et compl\'et\'ees par 86 exercices, tous corrig\'es.},
  isbn = {978-2-10-078080-8},
  langid = {french},
  keywords = {ml}
}

@article{bergstromImageQualityComputer2023,
  title = {Image Quality and Computer Vision Performance: Assessing the Effects of Image Distortions and Modeling Performance Relationships Using the General Image Quality Equation},
  shorttitle = {Image Quality and Computer Vision Performance},
  author = {Bergstrom, Austin C. and Messinger, David W.},
  year = {2023},
  month = mar,
  journal = {Journal of Electronic Imaging},
  volume = {32},
  pages = {023018},
  issn = {1017-9909},
  doi = {10.1117/1.JEI.32.2.023018},
  url = {https://ui.adsabs.harvard.edu/abs/2023JEI....32b3018B},
  urldate = {2023-08-07},
  abstract = {Substantial research has explored methods to optimize convolutional neural networks (CNNs) for tasks such as image classification and object detection, but research into the image quality drivers of computer vision performance has been limited. Additionally, there are indications that image degradations such as blur and noise affect human visual interpretation and machine interpretation differently. The general image quality equation (GIQE) predicts overhead image quality for human analysis using the National Image Interpretability Rating Scale, but no such model exists to predict image quality for interpretation by CNNs. Here, we assess the relationship between image quality variables and CNN performance. Specifically, we examine the impacts of resolution, blur, and noise on CNN performance for models trained with in-distribution and out-of-distribution distortions. Using two datasets, we observe that while generalization remains a significant challenge for CNNs faced with out-of-distribution image distortions, CNN performance against low visual quality images remains strong with appropriate training, indicating the potential to expand the design trade space for sensors providing data to computer vision systems. Additionally, we find that CNN performance predictions using the functional form of the GIQE can predict CNN performance as a function of image degradation, but we observe that the legacy form of the GIQE (from GIQE versions 3 and 4) does a better job of modeling the impact of blur/relative edge response in our experiments.},
  keywords = {ml},
  annotation = {ADS Bibcode: 2023JEI....32b3018B}
}

@article{brachmannComputationalExperimentalApproaches2017,
  title = {Computational and {{Experimental Approaches}} to {{Visual Aesthetics}}},
  author = {Brachmann, Anselm and Redies, Christoph},
  year = {2017},
  journal = {Frontiers in Computational Neuroscience},
  volume = {11},
  issn = {1662-5188},
  url = {https://www.frontiersin.org/articles/10.3389/fncom.2017.00102},
  urldate = {2023-08-01},
  abstract = {Aesthetics has been the subject of long-standing debates by philosophers and psychologists alike. In psychology, it is generally agreed that aesthetic experience results from an interaction between perception, cognition, and emotion. By experimental means, this triad has been studied in the field of experimental aesthetics, which aims to gain a better understanding of how aesthetic experience relates to fundamental principles of human visual perception and brain processes. Recently, researchers in computer vision have also gained interest in the topic, giving rise to the field of computational aesthetics. With computing hardware and methodology developing at a high pace, the modeling of perceptually relevant aspect of aesthetic stimuli has a huge potential. In this review, we present an overview of recent developments in computational aesthetics and how they relate to experimental studies. In the first part, we cover topics such as the prediction of ratings, style and artist identification as well as computational methods in art history, such as the detection of influences among artists or forgeries. We also describe currently used computational algorithms, such as classifiers and deep neural networks. In the second part, we summarize results from the field of experimental aesthetics and cover several isolated image properties that are believed to have a effect on the aesthetic appeal of visual stimuli. Their relation to each other and to findings from computational aesthetics are discussed. Moreover, we compare the strategies in the two fields of research and suggest that both fields would greatly profit from a joined research effort. We hope to encourage researchers from both disciplines to work more closely together in order to understand visual aesthetics from an integrated point of view.},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/4DKWHUWL/Brachmann et Redies - 2017 - Computational and Experimental Approaches to Visua.pdf}
}

@article{buttnerCorDeepSacroboscoDataset2022,
  title = {{{CorDeep}} and the {{Sacrobosco Dataset}}: {{Detection}} of {{Visual Elements}} in {{Historical Documents}}},
  shorttitle = {{{CorDeep}} and the {{Sacrobosco Dataset}}},
  author = {B{\"u}ttner, Jochen and Martinetz, Julius and {El-Hajj}, Hassan and Valleriani, Matteo},
  year = {2022},
  month = oct,
  journal = {Journal of Imaging},
  volume = {8},
  number = {10},
  pages = {285},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2313-433X},
  doi = {10.3390/jimaging8100285},
  url = {https://www.mdpi.com/2313-433X/8/10/285},
  urldate = {2023-08-10},
  abstract = {Recent advances in object detection facilitated by deep learning have led to numerous solutions in a myriad of fields ranging from medical diagnosis to autonomous driving. However, historical research is yet to reap the benefits of such advances. This is generally due to the low number of large, coherent, and annotated datasets of historical documents, as well as the overwhelming focus on Optical Character Recognition to support the analysis of historical documents. In this paper, we highlight the importance of visual elements, in particular illustrations in historical documents, and offer a public multi-class historical visual element dataset based on the Sphaera corpus. Additionally, we train an image extraction model based on YOLO architecture and publish it through a publicly available web-service to detect and extract multi-class images from historical documents in an effort to bridge the gap between traditional and computational approaches in historical studies.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/WLAD65M9/Büttner et al. - 2022 - CorDeep and the Sacrobosco Dataset Detection of V.pdf}
}

@article{cannyComputationalApproachEdge1986,
  title = {A {{Computational Approach}} to {{Edge Detection}}},
  author = {Canny, John},
  year = {1986},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-8},
  number = {6},
  pages = {679--698},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1986.4767851},
  abstract = {This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.},
  keywords = {ml}
}

@misc{carremansHandlingOverfittingDeep2019,
  title = {Handling Overfitting in Deep Learning Models},
  author = {Carremans, Bert},
  year = {2019},
  month = jan,
  journal = {Medium},
  url = {https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e},
  urldate = {2023-08-22},
  abstract = {Overfitting occurs when you achieve a good fit of your model on the training data, while it does not generalize well on new, unseen data\ldots},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/69537YEB/handling-overfitting-in-deep-learning-models-c760ee047c6e.html}
}

@book{cholletApprentissageProfondAvec2020a,
  title = {{L'apprentissage profond avec Python}},
  author = {Chollet, Fran{\c c}ois},
  year = {2020},
  month = jul,
  publisher = {{Machinelearning.fr}},
  address = {{Saint-Cyr-sur-Loire}},
  abstract = {L'apprentissage automatique a fait des progr\`es remarquables au cours des derni\`eres ann\'ees. Nous sommes pass\'es d'une reconnaissance quasi inutilisable de la parole et des images \`a une pr\'ecision quasi humaine, de machines qui ne pouvaient pas battre un joueur de Go un peu exp\'eriment\'e \`a la d\'efaite d'un champion du monde. Derri\`ere ces progr\`es se cache l'apprentissage profond \rule{1em}{1pt} une combinaison d'avanc\'ees th\'eoriques et pratiques qui permet une multitude d'applications intelligentes jusque-l\`a impossibles \`a r\'ealiser.L'apprentissage profond avec Python pr\'esente le domaine de l'apprentissage profond en utilisant le langage Python et la puissante biblioth\`eque Keras. R\'edig\'e par Fran\c{c}ois Chollet, cr\'eateur de Keras et chercheur en intelligence artificielle \`a Google, cet ouvrage construit votre compr\'ehension de l'apprentissage profond gr\^ace \`a des explications intuitives et des exemples pratiques. Vous explorerez des concepts et des pratiques stimulantes avec des applications de vision par ordinateur, de traitement du langage naturel et de mod\`eles g\'en\'eratifs. Lorsque vous aurez termin\'e, vous aurez les connaissances et les comp\'etences pratiques n\'ecessaires pour appliquer l'apprentissage profond \`a vos propres projets.Aucune exp\'erience pr\'ealable de Keras, de TensorFlow ou d'apprentissage automatique n'est requise. Les lecteurs ont besoin de comp\'etences Python interm\'ediaires.Le livre est imprim\'e sur du papier certifi\'e PEFC (Programme for the Endorsement of Forest Certification), les encres et techniques d'impression ont le label Imprim'Vert. Le Livre est imprim\'e en R\'egion Centre-Val de Loire. Une compensation volontaire carbone de 3kg par ouvrage (pesant 1,250kg) est vers\'ee \`a la Fondation GoodPlanet.org. Machinelearning.fr s'engage pour la transition environnementale.},
  isbn = {978-2-491-67400-7},
  langid = {french},
  keywords = {ml}
}

@unpublished{clericeYouActuallyLook2023,
  title = {You {{Actually Look Twice At}} It ({{YALTAi}}): Using an Object Detection Approach Instead of Region Segmentation within the {{Kraken}} Engine},
  shorttitle = {You {{Actually Look Twice At}} It ({{YALTAi}})},
  author = {Cl{\'e}rice, Thibault},
  year = {2023},
  month = apr,
  url = {https://enc.hal.science/hal-03723208},
  urldate = {2023-08-15},
  abstract = {Layout Analysis (the identification of zones and their classification) is the first step along line segmentation in Optical Character Recognition and similar tasks. The ability of identifying main body of text from marginal text or running titles makes the difference between extracting the work full text of a digitized book and noisy outputs. We show that most segmenters focus on pixel classification and that polygonization of this output has not been used as a target for the latest competition on historical document (ICDAR 2017 and onwards), despite being the focus in the early 2010s. We propose to shift, for efficiency, the task from a pixel classification-based polygonization to an object detection using isothetic rectangles. We compare the output of Kraken and YOLOv5 in terms of segmentation and show that the later severely outperforms the first on small datasets (1110 samples and below). We release two datasets for training and evaluation on historical documents as well as a new package, YALTAi, which injects YOLOv5 in the segmentation pipeline of Kraken 4.1.},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/ZZHHWQ4D/Clérice - 2023 - You Actually Look Twice At it (YALTAi) using an o.pdf}
}

@inproceedings{crowleyArtDetection2016,
  title = {The {{Art}} of {{Detection}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016 {{Workshops}}},
  author = {Crowley, Elliot J. and Zisserman, Andrew},
  editor = {Hua, Gang and J{\'e}gou, Herv{\'e}},
  year = {2016},
  volume = {9913},
  pages = {721--737},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46604-0_50},
  url = {http://link.springer.com/10.1007/978-3-319-46604-0_50},
  urldate = {2023-08-01},
  abstract = {The objective of this work is to recognize object categories in paintings, such as cars, cows and cathedrals. We achieve this by training classifiers from natural images of the objects. We make the following contributions: (i) we measure the extent of the domain shift problem for image-level classifiers trained on natural images vs paintings, for a variety of CNN architectures; (ii) we demonstrate that classification-by-detection (i.e. learning classifiers for regions rather than the entire image) recognizes (and locates) a wide range of small objects in paintings that are not picked up by image-level classifiers, and combining these two methods improves performance; and (iii) we develop a system that learns a region-level classifier on-the-fly for an object category of a user's choosing, which is then applied to over 60 million object regions across 210,000 paintings to retrieve localised instances of that category.},
  isbn = {978-3-319-46603-3 978-3-319-46604-0},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/QX3BSFBX/Crowley et Zisserman - 2016 - The Art of Detection.pdf}
}

@article{derpanisOverviewRANSACAlgorithm2010,
  title = {Overview of the {{RANSAC Algorithm}}},
  author = {Derpanis, Konstantinos G},
  year = {2010},
  month = may,
  langid = {english},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/M6YUG884/Derpanis - Overview of the RANSAC Algorithm.pdf}
}

@misc{dowlingGuideFileFormats2019,
  title = {Guide to {{File Formats}} for {{Machine Learning}}: {{Columnar}}, {{Training}}, {{Inferencing}}, and the {{Feature Store}}},
  shorttitle = {Guide to {{File Formats}} for {{Machine Learning}}},
  author = {Dowling, Jim},
  year = {2019},
  month = oct,
  journal = {Medium},
  url = {https://towardsdatascience.com/guide-to-file-formats-for-machine-learning-columnar-training-inferencing-and-the-feature-store-2e0c3d18d4f9},
  urldate = {2023-08-29},
  abstract = {TLDR; Most machine learning models are trained using data from files. This post is a guide to the popular file formats used in open source\ldots},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/7TX4HSZ3/guide-to-file-formats-for-machine-learning-columnar-training-inferencing-and-the-feature-store-.html}
}

@article{falomirCategorizingPaintingsArt2018,
  title = {Categorizing Paintings in Art Styles Based on Qualitative Color Descriptors, Quantitative Global Features and Machine Learning ({{QArt-Learn}})},
  author = {Falomir, Zoe and Museros, Lled{\'o} and Sanz, Ismael and {Gonzalez-Abril}, Luis},
  year = {2018},
  month = may,
  journal = {Expert Systems with Applications},
  volume = {97},
  pages = {83--94},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2017.11.056},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417417308126},
  urldate = {2023-08-01},
  abstract = {The QArt-Learn approach for style painting categorization based on Qualitative Color Descriptors (QCD), color similarity (SimQCD), and quantitative global features (i.e. average of brightness, hue, saturation and lightness and brightness contrast) is presented in this paper. k-Nearest Neighbor (k-NN) and support vector machine (SVM) techniques have been used for learning the features of paintings from the Baroque, Impressionism and Post-Impressionism styles. Specifically two classifiers are built, and two different parameterizations have been applied for the QCD. For testing QArt-Learn approach, the Painting-91 dataset has been used, from which the paintings corresponding to Vel\'azquez, Vermeer, Monet, Renoir, van Gogh and Gauguin were extracted, resulting in a set of 252 paintings. The results obtained have shown categorization accuracies higher than 65\%, which are comparable to accuracies obtained in the literature. However, QArt-Learn uses qualitative color names which can describe style color palettes linguistically, so that they can be better understood by non-experts in art since QCDs are aligned with human perception.},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/98JET9D2/S0957417417308126.html}
}

@inproceedings{fokaComputerVisionApplications2021,
  title = {Computer {{Vision Applications}} for {{Art History}}: {{Reflections}} and Paradigms for Future Research},
  shorttitle = {Computer {{Vision Applications}} for {{Art History}}},
  booktitle = {Proceedings of {{EVA London}} 2021},
  author = {Foka, Amalia F.},
  year = {2021},
  month = jul,
  pages = {73--80},
  publisher = {{BCS Learning \& Development}},
  doi = {10.14236/ewic/EVA2021.12},
  url = {https://www.scienceopen.com/hosted-document?doi=10.14236/ewic/EVA2021.12},
  urldate = {2023-08-01},
  abstract = {One of the contributing factors to the continuing debate among art historians over the use of computational methods in art history research is that they do not consider the core of today's art history research questions. The lack of close collaboration between the two involved research communities makes the definition of contemporary art-historical methods as well-defined computer vision problems extremely difficult. For that purpose, it is devised as a methodology to study articles in art history journals from a computer science perspective. The objective is to identify which image features art historians utilise within their research and describe them in immediate and meaningful terms to the computer vision research community. Finally, some paradigms that could serve as a new starting point for exploring how computer vision applications for art history can address the core of today's art history research are given.},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/LX3GPZTH/Foka - 2021 - Computer Vision Applications for Art History Refl.pdf}
}

@book{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}},
  url = {http://www.deeplearningbook.org},
  keywords = {ml}
}

@misc{linComprehensiveReviewImage2023,
  title = {A {{Comprehensive Review}} of {{Image Line Segment Detection}} and {{Description}}: {{Taxonomies}}, {{Comparisons}}, and {{Challenges}}},
  shorttitle = {A {{Comprehensive Review}} of {{Image Line Segment Detection}} and {{Description}}},
  author = {Lin, Xinyu and Zhou, Yingjie and Liu, Yipeng and Zhu, Ce},
  year = {2023},
  month = apr,
  number = {arXiv:2305.00264},
  eprint = {2305.00264},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.00264},
  url = {http://arxiv.org/abs/2305.00264},
  urldate = {2023-08-10},
  abstract = {Detection and description of line segments lay the basis for numerous vision tasks. Although many studies have aimed to detect and describe line segments, a comprehensive review is lacking, obstructing their progress. This study fills the gap by comprehensively reviewing related studies on detecting and describing two-dimensional image line segments to provide researchers with an overall picture and deep understanding. Based on their mechanisms, two taxonomies for line segment detection and description are presented to introduce, analyze, and summarize these studies, facilitating researchers to learn about them quickly and extensively. The key issues, core ideas, advantages and disadvantages of existing methods, and their potential applications for each category are analyzed and summarized, including previously unknown findings. The challenges in existing methods and corresponding insights for potentially solving them are also provided to inspire researchers. In addition, some state-of-the-art line segment detection and description algorithms are evaluated without bias, and the evaluation code will be publicly available. The theoretical analysis, coupled with the experimental results, can guide researchers in selecting the best method for their intended vision applications. Finally, this study provides insights for potentially interesting future research directions to attract more attention from researchers to this field.},
  archiveprefix = {arxiv},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/LSQMA4DK/Lin et al. - 2023 - A Comprehensive Review of Image Line Segment Detec.pdf;/home/jade/snap/zotero-snap/common/Zotero/storage/SMBYA2LF/2305.html}
}

@misc{moiraghiExplorerCorpusImages2018,
  type = {{Billet}},
  title = {{Explorer des corpus d'images. L'IA au service du patrimoine}},
  author = {Moiraghi, Eleonora and Moreux, Jean-Philippe},
  year = {2018},
  month = apr,
  journal = {Carnet de la recherche \`a la Biblioth\`eque nationale de France},
  url = {https://bnf.hypotheses.org/2809},
  urldate = {2023-08-06},
  abstract = {Suite aux ateliers \guillemotleft{} D\'ecrire, transcrire et diffuser un corpus documentaire h\'et\'erog\`ene : m\'ethodes, formats, outils \guillemotright{} et \guillemotleft ~G\'eolocalisation et spatialisation de documents patrimoniaux~\guillemotright, une troisi\`eme demi-journ\'ee d'\'etude a \'et\'e organis\'ee dans le cadre du...},
  langid = {french},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/EA5XSUVW/2809.html}
}

@inproceedings{monnierDocExtractorOfftheshelfHistorical2020,
  title = {{{docExtractor}}: {{An}} off-the-Shelf Historical Document Element Extraction},
  shorttitle = {{{docExtractor}}},
  booktitle = {2020 17th {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}} ({{ICFHR}})},
  author = {Monnier, Tom and Aubry, Mathieu},
  year = {2020},
  month = sep,
  eprint = {2012.08191},
  primaryclass = {cs},
  pages = {91--96},
  doi = {10.1109/ICFHR2020.2020.00027},
  url = {http://arxiv.org/abs/2012.08191},
  urldate = {2023-08-14},
  abstract = {We present docExtractor, a generic approach for extracting visual elements such as text lines or illustrations from historical documents without requiring any real data annotation. We demonstrate it provides high-quality performances as an off-the-shelf system across a wide variety of datasets and leads to results on par with state-of-the-art when fine-tuned. We argue that the performance obtained without fine-tuning on a specific dataset is critical for applications, in particular in digital humanities, and that the line-level page segmentation we address is the most relevant for a general purpose element extraction engine. We rely on a fast generator of rich synthetic documents and design a fully convolutional network, which we show to generalize better than a detection-based approach. Furthermore, we introduce a new public dataset dubbed IlluHisDoc dedicated to the fine evaluation of illustration segmentation in historical documents.},
  archiveprefix = {arxiv},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/G96IABYM/Monnier et Aubry - 2020 - docExtractor An off-the-shelf historical document.pdf;/home/jade/snap/zotero-snap/common/Zotero/storage/YKYGZPB4/2012.html}
}

@misc{monnierUnsupervisedLayeredImage2021,
  title = {Unsupervised {{Layered Image Decomposition}} into {{Object Prototypes}}},
  author = {Monnier, Tom and Vincent, Elliot and Ponce, Jean and Aubry, Mathieu},
  year = {2021},
  month = aug,
  number = {arXiv:2104.14575},
  eprint = {2104.14575},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.14575},
  url = {http://arxiv.org/abs/2104.14575},
  urldate = {2023-08-05},
  abstract = {We present an unsupervised learning framework for decomposing images into layers of automatically discovered object models. Contrary to recent approaches that model image layers with autoencoder networks, we represent them as explicit transformations of a small set of prototypical images. Our model has three main components: (i) a set of object prototypes in the form of learnable images with a transparency channel, which we refer to as sprites; (ii) differentiable parametric functions predicting occlusions and transformation parameters necessary to instantiate the sprites in a given image; (iii) a layered image formation model with occlusion for compositing these instances into complete images including background. By jointly learning the sprites and occlusion/transformation predictors to reconstruct images, our approach not only yields accurate layered image decompositions, but also identifies object categories and instance parameters. We first validate our approach by providing results on par with the state of the art on standard multi-object synthetic benchmarks (Tetrominoes, Multi-dSprites, CLEVR6). We then demonstrate the applicability of our model to real images in tasks that include clustering (SVHN, GTSRB), cosegmentation (Weizmann Horse) and object discovery from unfiltered social network images. To the best of our knowledge, our approach is the first layered image decomposition algorithm that learns an explicit and shared concept of object type, and is robust enough to be applied to real images.},
  archiveprefix = {arxiv},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/FFZTA77T/Monnier et al. - 2021 - Unsupervised Layered Image Decomposition into Obje.pdf;/home/jade/snap/zotero-snap/common/Zotero/storage/SM5I5PZE/2104.html}
}

@article{musikComputerVisionDigital2018,
  title = {Computer {{Vision}} and the {{Digital Humanities}}: {{Adapting Image Processing Algorithms}} and {{Ground Truth}} through {{Active Learning}}},
  shorttitle = {Computer {{Vision}} and the {{Digital Humanities}}},
  author = {Musik, Christoph and Zeppelzauer, Matthias},
  year = {2018},
  month = dec,
  journal = {VIEW Journal of European Television History and Culture},
  volume = {7},
  number = {14},
  pages = {59},
  issn = {2213-0969},
  doi = {10.18146/2213-0969.2018.jethc153},
  url = {https://www.viewjournal.eu/article/10.18146/2213-0969.2018.jethc153/},
  urldate = {2023-08-10},
  abstract = {Automated computer vision methods and tools offer new ways of analysing audio-visual material in the realm of the Digital Humanities (DH). While there are some promising results where these tools can be applied, there are basic challenges, such as algorithmic bias and the lack of sufficient transparency, one needs to carefully use these tools in a productive and responsible way. When it comes to the socio-technical understanding of computer vision tools and methods, a major unit of sociological analysis, attentiveness, and access for configuration (for both computer vision scientists and DH scholars) is what computer science calls ``ground truth''. What is specified in the ground truth is the template or rule to follow, e.g. what an object looks like. This article aims at providing scholars in the DH with knowledge about how automated tools for image analysis work and how they are constructed. Based on these insights, the paper introduces an approach called ``active learning'' that can help to configure these tools in ways that fit the specific requirements and research questions of the DH in a more adaptive and user-centered way. We argue that both objectives need to be addressed, as this is, by all means, necessary for a successful implementation of computer vision tools in the DH and related fields.},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/HQ4BHCDS/Musik et Zeppelzauer - 2018 - Computer Vision and the Digital Humanities Adapti.pdf}
}

@misc{salehAutomatedDiscoveryArtistic2014,
  title = {Toward {{Automated Discovery}} of {{Artistic Influence}}},
  author = {Saleh, Babak and Abe, Kanako and Arora, Ravneet Singh and Elgammal, Ahmed},
  year = {2014},
  month = aug,
  number = {arXiv:1408.3218},
  eprint = {1408.3218},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1408.3218},
  url = {http://arxiv.org/abs/1408.3218},
  urldate = {2023-08-30},
  abstract = {Considering the huge amount of art pieces that exist, there is valuable information to be discovered. Examining a painting, an expert can determine its style, genre, and the time period that the painting belongs. One important task for art historians is to find influences and connections between artists. Is influence a task that a computer can measure? The contribution of this paper is in exploring the problem of computer-automated suggestion of influences between artists, a problem that was not addressed before in a general setting. We first present a comparative study of different classification methodologies for the task of fine-art style classification. A two-level comparative study is performed for this classification problem. The first level reviews the performance of discriminative vs. generative models, while the second level touches the features aspect of the paintings and compares semantic-level features vs. low-level and intermediate-level features present in the painting. Then, we investigate the question "Who influenced this artist?" by looking at his masterpieces and comparing them to others. We pose this interesting question as a knowledge discovery problem. For this purpose, we investigated several painting-similarity and artist-similarity measures. As a result, we provide a visualization of artists (Map of Artists) based on the similarity between their works},
  archiveprefix = {arxiv},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/Y5H8DKEM/Saleh et al. - 2014 - Toward Automated Discovery of Artistic Influence.pdf;/home/jade/snap/zotero-snap/common/Zotero/storage/L3AUXBXL/1408.html}
}

@misc{shenLargeScaleHistoricalWatermark2019,
  title = {Large-{{Scale Historical Watermark Recognition}}: Dataset and a New Consistency-Based Approach},
  shorttitle = {Large-{{Scale Historical Watermark Recognition}}},
  author = {Shen, Xi and Pastrolin, Ilaria and Bounou, Oumayma and Gidaris, Spyros and Smith, Marc and Poncet, Olivier and Aubry, Mathieu},
  year = {2019},
  month = aug,
  number = {arXiv:1908.10254},
  eprint = {1908.10254},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1908.10254},
  url = {http://arxiv.org/abs/1908.10254},
  urldate = {2023-08-25},
  abstract = {Historical watermark recognition is a highly practical, yet unsolved challenge for archivists and historians. With a large number of well-defined classes, cluttered and noisy samples, different types of representations, both subtle differences between classes and high intra-class variation, historical watermarks are also challenging for pattern recognition. In this paper, overcoming the difficulty of data collection, we present a large public dataset with more than 6k new photographs, allowing for the first time to tackle at scale the scenarios of practical interest for scholars: one-shot instance recognition and cross-domain one-shot instance recognition amongst more than 16k fine-grained classes. We demonstrate that this new dataset is large enough to train modern deep learning approaches, and show that standard methods can be improved considerably by using mid-level deep features. More precisely, we design both a matching score and a feature fine-tuning strategy based on filtering local matches using spatial consistency. This consistency-based approach provides important performance boost compared to strong baselines. Our model achieves 55\% top-1 accuracy on our very challenging 16,753-class one-shot cross-domain recognition task, each class described by a single drawing from the classic Briquet catalog. In addition to watermark classification, we show our approach provides promising results on fine-grained sketch-based image retrieval.},
  archiveprefix = {arxiv},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/HXL6M9PN/Shen et al. - 2019 - Large-Scale Historical Watermark Recognition data.pdf;/home/jade/snap/zotero-snap/common/Zotero/storage/2SJG96UQ/1908.html}
}

@article{strienComputerVisionHumanities2022,
  title = {Computer {{Vision}} for the {{Humanities}}: {{An Introduction}} to {{Deep Learning}} for {{Image Classification}} ({{Part}} 1)},
  shorttitle = {Computer {{Vision}} for the {{Humanities}}},
  author = {van Strien, Daniel and Beelen, Kaspar and Wevers, Melvin and Smits, Thomas and McDonough, Katherine},
  year = {2022},
  month = aug,
  journal = {Programming Historian},
  url = {https://programminghistorian.org/en/lessons/computer-vision-deep-learning-pt1},
  urldate = {2023-08-10},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/T7JUD62Y/computer-vision-deep-learning-pt1.html}
}

@misc{UsingComputerVision,
  title = {Using Computer Vision Tools for Historical Newspaper Analysis: {{SIAMESE}} and {{Europeana Newspapers}}},
  shorttitle = {Using Computer Vision Tools for Historical Newspaper Analysis},
  journal = {Europeana PRO},
  url = {https://pro.europeana.eu/post/using-computer-vision-tools-for-historical-newspaper-analysis-siamese-and-europeana-newspapers},
  urldate = {2023-08-01},
  abstract = {An interview with Melvin Wevers, Digital Humanities researcher at KNAW, and Clemens Neudecker, coordinator of Europeana Newspapers},
  langid = {english},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/3U2T74KU/using-computer-vision-tools-for-historical-newspaper-analysis-siamese-and-europeana-newspapers.html}
}

@misc{vallerianiDetectingVisualElements2022,
  title = {Detecting {{Visual Elements}} in {{Historical Documents}}},
  author = {Valleriani, Matteo and B{\"u}ttner, Jochen and {El-Hajj}, Hassan and Martinetz, Julius},
  year = {2022},
  month = dec,
  url = {https://www.bifold.berlin/news-events/news/view/news-detail/detecting-visual-elements-in-historical-documents},
  urldate = {2023-08-10},
  abstract = {Historians are increasingly in need of digital tools to process and extract information from electronic copies of historical sources. BIFOLD scientists developed\&nbsp;YOLO (You Only Look Once).},
  keywords = {ml},
  file = {/home/jade/snap/zotero-snap/common/Zotero/storage/X4KZQNFV/detecting-visual-elements-in-historical-documents.html}
}
