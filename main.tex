\documentclass[a4paper,12pt,twoside]{book}
\usepackage[T1]{fontenc}
\usepackage{inputenc}
\usepackage{fontspec}
\usepackage{lmodern}
\usepackage[english,french]{babel}
\usepackage{xspace} % Pour la gestion des espaces après les commandes
\usepackage{csquotes} % Gestion des guillemets dans la biblio et citations
\usepackage[xetex]{graphicx} % Package pour gérer les images
\usepackage{caption}
\usepackage{subcaption} % Package pour les subfigures
\usepackage{float} % Gestion de la position des images
\usepackage{listings} % Blocs de code
\usepackage{xcolor} % Couleurs des blocs de code
\usepackage{pdfpages} % Insertion de PDF dans le fichier

% Couleurs des blocs de code
\definecolor{ferngreen}{rgb}{0.31, 0.47, 0.26}
\definecolor{codegray}{rgb}{0.44, 0.5, 0.56}
\definecolor{sinopia}{rgb}{0.8, 0.25, 0.04}
\definecolor{silver}{rgb}{0.95, 0.95, 0.95}
\definecolor{bondiblue}{rgb}{0.0, 0.58, 0.71}

%Style des blocs de code
\lstdefinestyle{code-memoire}{
	backgroundcolor=\color{silver},   
	commentstyle=\color{codegray},
	keywordstyle=\color{sinopia},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{ferngreen},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=code-memoire}

\usepackage{hyperref}
\hypersetup{%
	pdfauthor={Jade Norindr}
	pdfsubject={Mémoire TNAH — Le traitement des sources historiques par la vision artificielle}, 
	pdfkeywords={vision artificielle, apprentissage profond, API, histoire des sciences}
}

% Mise en page École des chartes
\usepackage[margin=2.5cm]{geometry} % marges
\usepackage{setspace}
\onehalfspacing % Interligne de 1.5
\setlength\parindent{1cm}

% Bibliographies
\usepackage{tocbibind}
\usepackage[backend=biber, sorting=nyt, style=enc, minbibnames=10, maxbibnames=10]{biblatex}
\addbibresource{bibliographie/humanum.bib}
\addbibresource{bibliographie/ml.bib}
\addbibresource{bibliographie/technique.bib}
\addbibresource{bibliographie/astronomie.bib}
\addbibresource{bibliographie/websites.bib}
\nocite{*}
\defbibnote{intro}{Cette bibliographie présente toutes les ressources utilisées, de tout type, citées ou non, par simple ordre alphabétique.}


\author{Jade Norindr – M2 TNAH — ENC}
\title{Le traitement des sources historiques par la vision artificielle. L'exemple des manuscrits d'astronomie de tradition ptoléméenne}

% Acronymes
\usepackage[automake, acronym, toc]{glossaries}
\makeglossaries

\setacronymstyle{short-long}
\newacronym{api}{API}{\emph{Application Programming Interface}}
\newacronym{bnf}{BnF}{Bibliothèque nationale de France}
\newacronym{cnn}{CNN}{Convolutional Neural Nets}
\newacronym{cpu}{CPU}{\textit{Central Processing Unit}}
\newacronym{csv}{CSV}{\emph{Comma-separated values}}
\newacronym{dips}{DIPS}{\emph{DISHAS Interactive Parameter Squeezer}}
\newacronym{dishas}{DISHAS}{\emph{Digital Information System for the History of Astral Sciences}}
\newacronym{dti}{DTI}{\emph{DISHAS Table Interface}}
\newacronym{eida}{EIDA}{\emph{Editing and analysing hIstorical astronomical Diagrams with Artificial intelligence}}
\newacronym{enc}{ENC}{École nationale des chartes}
\newacronym{enherit}{EnHerit}{\emph{Enhancing Heritage Image Databases}}
\newacronym{gpu}{GPU}{\textit{Graphics Processing Unit}}
\newacronym{html}{HTML}{\emph{HyperText Markup Language}}
\newacronym{htr}{HTR}{\emph{Handwritten Text Recognition}}
\newacronym{http}{HTTP}{\emph{Hypertext Transfer Protocol}}
\newacronym{https}{HTTPS}{\emph{Hypertext Transfer Protocol Secure}}
\newacronym{iiif}{IIIF}{\emph{International Image Interoperability Framework}}
\newacronym{imagine}{IMAGINE}{Laboratoire d’Informatique Gaspard Monge}
\newacronym{inha}{INHA}{Institut national d'histoire de l'art}
\newacronym{iscd}{ISCD}{Institut des sciences du calcul et des données}
\newacronym{json}{JSON}{\emph{JavaScript Object Notation}}
\newacronym{ransac}{RANSAC}{\textit{RANdom SAmple Consensus}}
\newacronym{rest}{REST}{\textit{Representational state transfer}}
\newacronym{rmn}{Rmn-Grand Palais}{Réunion des musées nationaux-Grand Palais}
\newacronym{ssh}{SSH}{\textit{Secure Shell}}
\newacronym{svg}{SVG}{\emph{Scalable Vector Graphics}}
\newacronym{syrte}{SYRTE}{Systèmes de Référence Temps-Espace}
\newacronym{tei}{TEI}{\textit{Text Encoding Initiative}}
\newacronym{uri}{URI}{\emph{Uniform Resource Identifier}}
\newacronym{url}{URL}{\textit{Uniform Resource Locator}}
\newacronym{vhs}{VHS}{Vision artificielle et analyse Historique de la circulation de l'illustration Scientifique}
\newacronym{xml}{XML}{\emph{eXtensible Markup Language}}
\newacronym{yolo}{YOLO}{\textit{You Only Look Once}}

% Commandes
\newcommand{\api}{\gls{api}\xspace}
\newcommand{\bnf}{\gls{bnf}\xspace}
\newcommand{\cnn}{\gls{cnn}\xspace}
\newcommand{\cpu}{\gls{cpu}\xspace}
\newcommand{\cv}{\textit{computer vision}\xspace}
\newcommand{\dishas}{\gls{dishas}\xspace}
\newcommand{\dl}{\textit{deep learning}\xspace}
\newcommand{\docex}{docExtractor\xspace}
\newcommand{\eida}{\gls{eida}\xspace}
\newcommand{\enc}{\gls{enc}\xspace}
\newcommand{\enherit}{\gls{enherit}\xspace}
\newcommand{\exapi}{extractorAPI\xspace}
\newcommand{\gpu}{\gls{gpu}\xspace}
\newcommand{\http}{\gls{http}\xspace}
\newcommand{\https}{\gls{https}\xspace}
\newcommand{\ia}{intelligence artificielle\xspace}
\newcommand{\iiif}{\gls{iiif}\xspace}
\newcommand{\imagine}{\gls{imagine}\xspace}
\newcommand{\inha}{\gls{inha}\xspace}
\newcommand{\json}{\gls{json}\xspace}
\newcommand{\ml}{\textit{machine learning}\xspace}
\newcommand{\ponts}{École des Ponts ParisTech\xspace}
\newcommand{\rest}{\gls{rest}\xspace}
\newcommand{\ssh}{\gls{ssh}\xspace}
\newcommand{\svg}{\gls{svg}\xspace}
\newcommand{\uri}{\gls{uri}\xspace}
\newcommand{\URL}{\gls{url}\xspace}
\newcommand{\vhs}{\gls{vhs}\xspace}
\newcommand{\yolo}{\gls{yolo}\xspace}
\newcommand{\yolov}{YOLOv5\xspace}
\newcommand{\ist}{\textsc{i}\ieme{}\xspace}
\newcommand{\ii}{\textsc{ii}\ieme{}\xspace}
\newcommand{\viii}{\textsc{viii}\ieme{}\xspace}
\newcommand{\ix}{\textsc{ix}\ieme{}\xspace}
\newcommand{\xie}{\textsc{xi}\ieme{}\xspace}
\newcommand{\xii}{\textsc{xii}\ieme{}\xspace}
\newcommand{\xiii}{\textsc{xiii}\ieme{}\xspace}
\newcommand{\xv}{\textsc{xv}\ieme{}\xspace}
\newcommand{\xvi}{\textsc{xvi}\ieme{}\xspace}
\newcommand{\xviii}{\textsc{xviii}\ieme{}\xspace}
\newcommand{\jc}{av. J.-C.\xspace}
\newcommand{\ma}{Moyen-Âge\xspace}
\def\cdt{\kern-0.5pt\ensuremath\cdot\kern-0.5pt}

% Page à blanc sans en-tête et bas de page
\newcommand{\clearemptydoublepage}{\newpage{\pagestyle{empty}\cleardoublepage}}

% Pour des chapitres non numérotées dans la table des matière
\newcommand\chapterNo[1]{
  \chapter*{#1}
  \markright{\MakeUppercase{#1}}
}

\begin{document}
\onehalfspacing
\frontmatter

    \include{templates/page-titre}

    \thispagestyle{empty}	
    \cleardoublepage
	
    \include{templates/resume}
	
    \chapterNo{Remerciements}
    \addcontentsline{toc}{chapter}{Remerciements}
    
    Mes remerciements vont, en premier lieu, à toutes les personnes qui m'ont accompagnée lors de cette année de Master.
    
    Je tiens à adresser ma reconnaissance à mon directeur, Maxime Challon, pour son encadrement et pour ses conseils précieux tout au long de la rédaction de ce mémoire, qui ont permis son aboutissement. Je remercie chaleureusement Ségolène Albouy pour le rôle de tutrice qu'elle a joué lors de ce stage, pour sa bienveillance et son implication, qui m'ont permis de penser ce mémoire dans les meilleures conditions possible. Je remercie également Matthieu Husson pour la confiance qu'il m'a accordée, qui a rendu ce stage si enrichissant, et l'ensemble de l'équipe d'Histoire des sciences de l'Observatoire de Paris pour leur accueil.
    
    Merci ma famille et mes ami$\cdot$e$\cdot$s pour leur soutien lors de la rédaction de ce mémoire, à Ethan et Tara pour nos sessions de travail. Je remercie, enfin, Elsa et Arthur pour leurs relectures attentives.

    \printbibliography[keyword={DH},title={Humanités numériques}]
    \clearemptydoublepage
    \printbibliography[keyword={ml},title={\textit{Machine learning}, \textit{deep learning} et vision artificielle}]
    \clearemptydoublepage
    \printbibliography[keyword={technique},title={Documentation technique et méthode}]
    \clearemptydoublepage
    \printbibliography[keyword={astronomie},title={Histoire de l'astronomie}]
    \clearemptydoublepage
    
    \chapterNo{Introduction}
    \addcontentsline{toc}{chapter}{Introduction}

    \clearemptydoublepage

\mainmatter

    \part{Construire un corpus de numérisations pour le traitement par vision artificielle}
        \chapter{Le projet EiDA}
        Cette partie a pour objectif de revenir sur le contexte institutionnel du projet \eida, projet de recherche mené à l'Observatoire de Paris par l'équipe d'histoire des science du laboratoire \acrshort{syrte}. Ce projet a pour sujet d'étude les diagrammes astronomiques de tradition ptoléméenne : cette partie vise ainsi à contextualiser d'un point de vue historique le corpus du projet, et à en expliciter les bornes chronologiques et géographiques.
        
                \section{Contexte et objectifs du projet}
                    \input{templates/partie1/chap1/section1}

                \section{Sources primaires}
                    \input{templates/partie1/chap1/section2}
		\\
		
		\eida est un projet aux bornes chronologiques, géographiques et thématiques vastes, pour permettre une étude sur un temps long et dans un contexte global de la circulation des diagrammes astronomiques et des théories scientifiques qui les accompagnent. Dans cette démarche, proposer une étude quantitative, traitant un grand nombre de sources, permet de mettre en avant des motifs, connexions et évolutions en accord avec la diffusion afro-eurasienne des idées développées par Ptolémée. Les bornes définies dans cette partie permettent de délimiter un corpus d'images -- numérisations d'ouvrages manuscrits ou imprimés -- qui en tant d'objets numériques présentent également leurs propres problématiques, sur lesquelles revient la partie suivante.
        \clearemptydoublepage
        
        \chapter{Images et interopérabilité}
        Les sources étudiées dans ce mémoire sont des sources iconographiques : cette partie vise à revenir sur les formats, ressources et méthodes pour le traitement des images en ligne, de la production de la ressource à sa publication. Les images digitales présentent des enjeux spécifiques, du point de vue de la technique, du droit et de la disponibilité. La mise en ligne et la diffusion d'une image fait suite à une longue chaîne de traitement qui a pour point de départ un objet matériel, et soulève des questionnements divers. Les spécifications \iiif tente de répondre à un certain nombre de problématiques liées à la présence en ligne de ressources iconographiques, et cette partie vise donc à présenter les solutions, possibilités et limites offertes par ce standard.
        
                \section{L’image comme source}
                    \input{templates/partie1/chap2/section1}
            
                \section{\label{stardardIiif}Le standard IIIF}
                    \input{templates/partie1/chap2/section2}
        \\
        
        Les sources iconographiques sont soumises à un ensemble de restrictions, du point de vue du format, des métadonnées ou des droits, qui manquent encore d'une uniformité internationale et entre institutions qui rendrait fluide le partage de ces ressources sur Internet et entre les projets de recherche. Dans un projet impliquant l'utilisation d'algorithmes de vision artificielle, qui repose alors sur le traitement d'un volume important d'image, la mise en ligne de ces documents est un enjeux crucial, sur lequel repose d'une part la possibilité de constituer un corpus exploitable, ainsi que la publication des résultats du projet, qui peuvent également prendre la forme d'images numériques. Dans une optique de science ouverte, l'utilisation de standards et d'outils tels que ceux développés par le consortium \iiif permet d'assurer le partage d'images et de données respectant les mêmes formats, exploitables avec des outils libres, et d'avancer vers une abolition des silos de données, qui faciliterait notamment la construction de corpus massifs pour l'apprentissage machine. 
        \clearemptydoublepage
        
        \chapter[Corpus historiques et jeux de données]{Corpus historiques et jeux de données pour l’apprentissage machine}
        
        La vision artificielle et l'apprentissage machine permet le traitement de corpus d'images massifs par des méthodes quantitatives qui permettent aux historiens de traiter un volume de données bien plus important qu'une approche manuelle, ouvrant ainsi la voie à de nouvelles approches. Cette partie revient sur les bonnes pratiques à mettre en place afin d'assurer la pertinence de l'utilisation de ces outils, et d'en faire des traitements efficaces en accord avec les ambitions des projets.
        
                \section{Dimensions et cadre}
                    \input{templates/partie1/chap3/section1}
            
                \section{\label{objectifsPossibilites}Objectifs scientifiques et possibilités numériques}
                    \input{templates/partie1/chap3/section2}
        \\
        
        Le \dl et la vision artificielle permettent aux projets de recherche en histoire et en histoire de l'art d'envisager de nouvelles approches des sources, à l'aide de traitement automatisés qui offrent de nouvelles méthodes de navigation de corpus d'images massifs : de la détection à l'édition, les outils produits redéfinissent les étapes de traitement des sources, et il est ainsi nécessaire d'intégrer aux pratiques des chercheurs des méthodes spécifiques à ce type d'approche, et notamment pour la création de jeu de données d'entraînement, à la base du développement de tout modèle de \ml. Cette intégration passe notamment par la rédaction de documentation, ainsi qu'un dialogue entre les équipes de recherche en vision artificielle et les équipes d'historiens. Ce dialogue permet d'établir les besoins de chacun, et de développer des outils techniques qui répondent aux besoins scientifiques de manière pertinente, en développant des modèles qui, malgré leurs limites, rejoignent aux mieux les attentes des sciences historiques.
        \clearemptydoublepage


    \part{De l’image à l’objet : intégrer l’apprentissage profond au traitement des sources historiques}
        \chapter[L'apprentissage profond]{Principes et utilisation de l’apprentissage profond}
        
        L'apprentissage profond est un sous-domaine de l'apprentissage automatique basé sur l'apprentissage de couches successives de représentation. Le nombre de couches définit la profondeur du modèle : de nos jours, l'apprentissage profond compte plusieurs dizaines à plusieurs centaines de couches, qui apprennent toutes automatiquement à l'aide de données d'apprentissage\footcite{cholletApprentissageProfondAvec2020a}. Cette approche est au cœur des modèles de vision artificielle dont nous parlons dans ce mémoire, qui reposent sur des réseaux de neurones qui constituent ces couches superposées permettant un apprentissage des représentations à partir de données fournies.
        
                \section{Réseaux de neurones et \textit{computer vision}}
                    \input{templates/partie2/chap4/section1}
            
                \section[Modèles de vision \textit{off-the-shelf}]{Modèles de détection \textit{off-the-shelf} : outils libres pour l'extraction d’objets}
                    \input{templates/partie2/chap4/section2}
        \\
		
		Les modèles de vision \textit{off-the-shelf} ouvrent à des projets divers la possibilité d'intégrer la vision artificielle à leurs méthodologies, en réduisant le coût humain et temporel du développement d'un modèle de \textit{deep learning} par la mise à disposition en accès libre d'outils déjà performants, qu'il est possible d'entraîner pour les ajuster à des données spécifiques. Pour la détection d'objets, des jeux de données en accès libre tels qu'ImageNet permettent un pré-entraînement de ces modèles \textit{off-the-shelf}, qui apprennent alors des caractéristiques larges qui peuvent être précisées par un entraînement sur des sources plus spécifiques, en nécessitant un volume de données moins important qu'un modèle créé de zéro : ces modèles de détection \textit{off-the-shelf} présentent ainsi une solution aux limites que peuvent présenter les sources historiques en termes de volume des données disponibles, et permettent également aux projets de se construire sur des bases solides, sans allouer de ressources à la création d'outils déjà existants, déjà performants, pour des tâches telles que la détection d'objet qui font partie des tâches canoniques de la vision par ordinateur.
        \clearemptydoublepage
        
        \chapter[Construire une plateforme pour la détection]{Construire une plateforme pour la détection : outils, interfaces et modèles de données}
        
        L'intégration du \textit{deep learning} aux pratiques des chercheurs en sciences historiques passe par le développement d'une plateforme qui leur permet d'exploiter simplement ces outils pour traiter leurs sources. Cette plateforme à interface graphique doit être adaptée aux sources traitées, et pensée pour intégrer toutes les fonctionnalités souhaitées. En s'appuyant sur les développements réalisés par les projets \eida et \vhs, cette partie revient sur la construction d'une application  et d'une \api dédiées à la détection de diagrammes et images scientifiques dans les numérisations d'ouvrages manuscrits ou imprimés, avec une interface pour la correction de la détection : nous évoquons ainsi les méthodes de développement, les réflexions liées aux données, les besoins matériels liés à l'utilisation d'algorithmes de vision, et le développement d'une \api pour lancer l'inférence sur un \gpu. 
         
                \section{Penser une application pour la détection d'objets}
                    \input{templates/partie2/chap5/section1}
            
                \section{Annoter sur un GPU : extractorAPI}
                    \input{templates/partie2/chap5/section2}
        \\
        
        L'\textit{open source}\footnote{Le terme d'\textit{open source}, ou code source ouvert, désigne la pratique de mise à disposition du code source d'un logiciel sous licence libre, pour permettre gratuitement sa réutilisation, sa distribution et sa modification.} est une préoccupation centrale lors du développement d'outil pour les projets de recherche : il est en effet souhaitable de produire, dans la mesure du possible, des applications réutilisables par des projets futurs, pour assurer une continuité dans les travaux produits sans que se démultiplient les développements d'outils similaires. Ainsi, le remploi du code est au cœur des ambitions des équipes d'ingénierie : l'intégration de l'apprentissage profond à des projets d'histoire nécessite des outils spécifiques, construits autour de ces traitements des sources par l'\ia, qui restent néanmoins similaires -- ou comparables dans leurs besoins techniques -- d'un projet à l'autre. L'utilisation d'un \gpu pour la vision artificielle compte parmi les besoins matériels qui se traduisent entre les projets, ainsi, dans une optique de programmation modulaire, une \api externe à l'application \vhs/\eida a été développée pour répondre à ce besoin, sans être hautement spécifique aux besoins de ces projets comme pourrait l'être un module intégré à l'application. \exapi, l'\api développée dans ce cadre, est ainsi conçue, dès son cahier des charges, pour répondre aux besoins du projet \eida tout en restant réemployable dans d'autres contextes -- sont ainsi utilisés, pour sa construction, des outils libres qui en standardisent le développement, avec la vocation de produire une \api aussi flexible que robuste.
        \clearemptydoublepage
            
      	\chapter[\textit{Computer vision} et pratiques des chercheurs]{Intégrer la vision artificielle aux pratiques des chercheurs}
      	
      	Le chapitre précédent établit, du point de vue du \textit{back end}, les besoins techniques pour l'intégration de la vision artificielle à une chaîne de traitement des sources historiques : ces préoccupations en termes d'architecture et de puissance de calcul sont celles des équipes d'ingénierie, et existent en parallèle des préoccupations qui concernent plus directement les utilisateurs, et impactent réellement les pratiques des chercheurs. Ce chapitre revient ainsi sur la chaîne de traitement des sources historiques dans sa totalité, depuis la numérisation fournie par l'utilisateur jusqu'aux résultats retournés pour le traitement par les chercheurs, et sur les échanges avec les utilisateurs, en termes d'interface mais également en termes de médiation.
               
                \section[\textit{Workflow} de traitement des sources]{De la numérisation à l’annotation : l'automatisation du traitement des sources}
                    \input{templates/partie2/chap6/section1}
             
                \section{Médiation et documentation}
                    \input{templates/partie2/chap6/section2}
        \\
        
        L'intégration de la vision artificielle aux pratiques des chercheurs en humanités passe ainsi par l'établissement d'une chaîne de traitement qui trouve un équilibre entre besoins scientifiques et possibilités numériques, en mêlant étapes automatiques et correction manuelle pour s'assurer de la pertinence des données qui en découlent. L'automatisation n'est jamais totale : le traitement de sources historiques ne peut s'effectuer sans un regard critique, humain, qui contrebalance les limites de l'\ia qui ne permettent pas d'appréhender les objets étudiés sous tous les aspects nécessaires à prendre en compte dans le cadre d'une recherche historique. L'apprentissage profond et la vision artificielle s'insèrent donc dans les pratiques de chercheurs en histoire par le biais d'outils et d'interfaces qui appellent à leur contribution, à leurs corrections, et à leur regard sur les résultats produits par des algorithmes : les ingénieurs ont pour rôle, dans ce contexte, de développer des outils et pratiques qui permettent ce lien entre les méthodes et techniques du \dl et les chercheurs en humanités, et d'établir une médiation fluide qui assure la pertinence du travail produit. Au-delà de la communication avec les chercheurs, utilisateurs des interfaces, il est du devoir des ingénieurs d'assurer la pérennité des outils créés par une documentation destinée à d'autres développeurs, pour permettre la maintenance du code, ou son réemploi dans des projets extérieurs : ces bonnes pratiques assurent ainsi la vie des développements produits, et leur bonne intégration dans un contexte de recherche plus large, en tant qu'outils libres pour le traitement des sources.
        \clearemptydoublepage

    \part{Perspectives pour le traitement des sources : vers un outil pour l’édition et la recherche}
        \chapter[Éditer des diagrammes]{Éditer des diagrammes : vectorisation et édition critique}
        La partie précédente s'attache à décrire l'intégration des techniques et méthodes de la vision artificielle aux pratiques des chercheurs, en s'appuyant notamment sur le développement d'outils pour l'utilisation d'algorithmes de détection d'objets. Tâche canonique de la vision artificielle, la détection d'objets dans les images est une pratique établie, dont l'intérêt pour la navigation des corpus est avéré par un nombre certain de projets récents, tels que CorDeep\footcite{CorDeep}, ayant produit et publié des applications intégrant des algorithmes de détection d'objets pour le traitement des sources.
        
        La vision artificielle est riche en possibilités : au-delà des outils de détection plus accessibles -- dont des modèles \textit{off-the-shelf} existent pour les projets qui souhaiteraient l'employer -- les traitements automatiques pour l'édition ou pour l'étude critique des sources sont considérés et étudiés. La détection d'objet n'est ainsi pas considérée comme une finalité, mais comme une première étape dans une chaîne de traitement qui intègre des étapes automatique de détection de similarités, de \textit{clustering} ou de vectorisation\footnote{Par vectorisation, nous entendons la transcription de numérisations de diagrammes géométriques en images \svg.}, offrant de nouvelles perspectives dans le cadre de l'intégration de la vision artificielle à l'étude des sources. Pour des projets portés sur l'illustration scientifique, tels que le projet \eida, la vectorisation présente un intérêt particulier, ouvrant des possibilités en termes d'édition automatique des diagrammes.
        
                \section{Édition numérique des diagrammes astronomiques}
                    \input{templates/partie3/chap7/section1}
            
                \section[De l’image aux vecteurs]{\label{vectorEdition}De l’image aux vecteurs : la vision artificielle pour l’édition numérique}
                    \input{templates/partie3/chap7/section2}
        \\
        
        La vectorisation automatique de diagrammes géométriques ouvre de nombreuses perspectives en termes de soutien à la recherche, et permet d'envisager des outils et interfaces à destination des chercheurs qui proposeraient d'automatiser des tâches autrement chronophage. La transcription des diagrammes, en lien avec l'histoire récente de l'édition critique de ce type d'illustration, fait l'objet d'un nombre restreint -- si ce n'est inexistant -- de normes qui en encadrent la pratique : les méthodes appliquées pour la transcription et l'édition numérique sont donc chronophages, variées, spécifiques aux projets qui les emploient, et les objets informatiques générés ne sont pas nécessairement manipulables ou interopérables. Le \svg, format libre, est particulièrement appropriés pour l'étude des diagrammes : la transformation d'une numérisation en objet vectoriel devient alors une étape clé de la chaîne de traitement. Manuellement, cette dernière est chronophage, et demande la connaissance d'outils spécifiques pour cette pratique. Ainsi, l'intégration à cette chaîne de traitement d'algorithmes de \cv permet de l'envisager comme une pratique systématique, rendant possible la transcription d'un volume important d'images en un temps moindre. La création d'outils libres, accessibles et aisés à prendre en main intégrant ces méthodes a vocation à faciliter cette étape chronophage du processus d'édition, et de permettre une analyse des diagrammes tirant réellement parti des avantages du numérique.    
        \clearemptydoublepage
        
        \chapter{Détection de similarité et \textit{clustering}}
	             \section{Trouver des motifs dans les données : le \textit{clustering}}
        			\input{templates/partie3/chap8/section1}
        			
                \section{\textit{Similarity retrieval} et étude de la circulation des idées}
                    \input{templates/partie3/chap8/section2}
            
        \clearemptydoublepage
    
    \chapterNo{Conclusion}
    Étudier et exploiter les résultats automatiques : limites et perspectives pour les sciences historiques
    \addcontentsline{toc}{chapter}{Conclusion}
    \clearemptydoublepage
    
\appendix
    \part*{Annexes}	
    \addcontentsline{toc}{part}{Annexes}
    
    \chapter[Prepare custom data for training]{\label{YOLOv5Training}Prepare custom data for training using the YOLOv5 workflow}
	    \input{templates/annexes/yolotraining}
	    \clearemptydoublepage
    \chapter[Modèles de données EIDA/VHS]{\label{eidaDataModels}Modèles de données des applications VHS et EIDA}
	    \input{templates/annexes/datamodel}
     	\clearemptydoublepage
    \chapter{\label{exapiCahier}extractorAPI : cahier des charges}
	    \input{templates/annexes/exapicahier}
	    \clearemptydoublepage
	\chapter{\label{yoloScript}Script YOLOv5 pour lancer la détection d'objet}
		Le script suivant est une reproduction du script \texttt{detect\_vhs.py} utilisé dans \exapi pour lancer la détection d'objet dans les images envoyées par l'application : \url{https://github.com/jnorindr/extractorAPI/blob/main/yolov5/detect_vhs.py}
		\input{templates/annexes/yoloscript}
	    \clearemptydoublepage
	\chapter{\label{DHSeminar}DH Seminar: extractorAPI}
		La présentation suivante constitue le support d'un séminaire d'humanités numériques donné le 13 juin 2023 en présence de l'équipe du projet \eida. Le séminaire visait à présenter l'\api développée dans le cadre du stage.
		\includepdf[nup=1x2,pages=-, scale=0.8, delta=0 10mm]{templates/annexes/dhseminar.pdf}
	    \clearemptydoublepage
	\chapter[Annotate images with extractorAPI]{\label{exapiAnno}Annotate images from EiDA with extractorAPI}
		Ce document est extrait du Wiki de l'\api, disponible sur GitHub en accompagnement du code. Il a été rédigé pour documenter les interactions entre \exapi et l'application \eida, pour faciliter la réutilisation de l'\api par des projets tiers. Le document est disponible à l'adresse suivante : \url{https://github.com/jnorindr/extractorAPI/wiki/Annotate-images-from-EiDA-with-extractorAPI}
		\input{templates/annexes/exapianno}
		\clearemptydoublepage

\backmatter
    \printacronyms[title=Liste des acronymes,toctitle=Acronymes]
    \printglossary
    \listoffigures
    \clearemptydoublepage
    \tableofcontents
\end{document}