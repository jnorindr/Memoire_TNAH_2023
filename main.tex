\documentclass[a4paper,12pt,twoside]{book}
\usepackage[T1]{fontenc}
\usepackage{inputenc}
\usepackage{fontspec}
\usepackage{lmodern}
\usepackage[english,french]{babel}
\usepackage{xspace} % pour la gestion des espaces après les commandes
%\usepackage{minted} % colored source code
\usepackage{csquotes} % Gestion des guillemets dans la biblio
\usepackage[xetex]{graphicx} %Package pour gérer les images
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float} %Gestion de la position des images
\usepackage{listings}
\usepackage{xcolor}

%Couleurs des blocs de code
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Style des blocs de code
\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}

\usepackage{hyperref}
\hypersetup{%
	pdfusetitle, 
	pdfauthor={Jade Norindr}
	pdfsubject={Mémoire TNAH — Le traitement des sources historiques par la vision artificielle}, 
	pdfkeywords={vision artificielle}
}

% Mise en page École des chartes
\usepackage[margin=2.5cm]{geometry} % marges
\usepackage{setspace}
\onehalfspacing % interligne de 1.5
\setlength\parindent{1cm}

\usepackage{tocbibind}
\usepackage[backend=biber, sorting=nyt, style=enc]{biblatex}
\addbibresource{bibliographie/bibliographie.bib}
%\nocite{*}
\defbibnote{intro}{Cette bibliographie présente toutes les ressources utilisées, de tout type, citées ou non, par simple ordre alphabétique.}


\author{Jade Norindr – M2 TNAH — ENC}
\title{Le traitement des sources historiques par la vision artificielle. L'exemple des manuscrits d'astronomie de tradition ptoléméenne}

% Acronymes
\usepackage[automake, acronym, toc]{glossaries}
\makeglossaries

\setacronymstyle{short-long}
\newacronym{api}{\textsc{api}}{\emph{Application Programming Interface}}
\newacronym{eida}{\textsc{eida}}{\emph{Editing and analysing hIstorical astronomical Diagrams with Artificial intelligence}}
\newacronym{dishas}{\textsc{dishas}}{\emph{Digital Information System for the History of Astral Sciences}}
\newacronym{dti}{\textsc{dti}}{\emph{DISHAS Table Interface}}
\newacronym{dips}{\textsc{dips}}{\emph{DISHAS Interactive Parameter Squeezer}}
\newacronym{xml}{\textsc{xml}}{\emph{eXtensible Markup Language}}
\newacronym{csv}{\textsc{csv}}{\emph{Comma-separated values}}
\newacronym{html}{\textsc{html}}{\emph{HyperText Markup Language}}
\newacronym{htr}{\textsc{htr}}{\emph{Handwritten Text Recognition}}
\newacronym{imagine}{\textsc{imagine}}{Laboratoire d’Informatique Gaspard Monge}
\newacronym{vhs}{\textsc{vhs}}{Vision artificielle et analyse Historique de la circulation de l'illustration Scientifique}
\newacronym{syrte}{\textsc{syrte}}{Systèmes de Référence Temps-Espace}
\newacronym{iscd}{\textsc{iscd}}{Institut des sciences du calcul et des données}
\newacronym{bnf}{BnF}{Bibliothèque nationale de France}
\newacronym{inha}{\textsc{inha}}{Institut national d'histoire de l'art}
\newacronym{enc}{\textsc{enc}}{École nationale des chartes}
\newacronym{rmn}{Rmn-Grand Palais}{Réunion des musées nationaux-Grand Palais}
\newacronym{iiif}{\textsc{iiif}}{\emph{International Image Interoperability Framework}}
\newacronym{http}{\textsc{http}}{\emph{Hypertext Transfer Protocol}}
\newacronym{https}{\textsc{https}}{\emph{Hypertext Transfer Protocol Secure}}
\newacronym{uri}{\textsc{uri}}{\emph{Uniform Resource Identifier}}
\newacronym{json}{\textsc{json}}{\emph{JavaScript Object Notation}}
\newacronym{svg}{\textsc{svg}}{\emph{Scalable Vector Graphics}}
\newacronym{enherit}{EnHerit}{\emph{Enhancing Heritage Image Databases}}
\newacronym{cnn}{\textsc{cnn}}{Convolutional Neural Nets}
\newacronym{yolo}{\textsc{yolo}}{\textit{You Only Look Once}}
\newacronym{gpu}{\textsc{gpu}}{\textit{Graphics Processing Unit}}
\newacronym{cpu}{\textsc{cpu}}{\textit{Central Processing Unit}}
\newacronym{url}{\textsc{url}}{\textit{Uniform Resource Locator}}
\newacronym{ssh}{\textsc{ssh}}{\textit{Secure Shell}}
\newacronym{rest}{\textsc{rest}}{\textit{Representational state transfer}}


% Acronymes en petites capitales dans la liste des acronymes
\usepackage{enumitem}
\setlist[description]{labelwidth=2em, labelsep=.5em, font=\normalfont}

% Commandes
\newcommand{\rest}{\gls{rest}\xspace}
\newcommand{\gpu}{\gls{gpu}\xspace}
\newcommand{\cpu}{\gls{cpu}\xspace}
\newcommand{\ssh}{\gls{ssh}\xspace}
\newcommand{\URL}{\gls{url}\xspace}
\newcommand{\yolo}{\gls{yolo}\xspace}
\newcommand{\yolov}{\textsc{yolo}v5\xspace}
\newcommand{\docex}{docExtractor\xspace}
\newcommand{\exapi}{extractorAPI\xspace}
\newcommand{\cnn}{\gls{cnn}\xspace}
\newcommand{\imagine}{\gls{imagine}\xspace}
\newcommand{\enherit}{\gls{enherit}\xspace}
\newcommand{\svg}{\gls{svg}\xspace}
\newcommand{\json}{\gls{json}\xspace}
\newcommand{\uri}{\gls{uri}\xspace}
\newcommand{\iiif}{\gls{iiif}\xspace}
\newcommand{\http}{\gls{http}\xspace}
\newcommand{\https}{\gls{https}\xspace}
\newcommand{\bnf}{\gls{bnf}\xspace}
\newcommand{\inha}{\gls{inha}\xspace}
\newcommand{\enc}{\gls{enc}\xspace}
\newcommand{\ponts}{École des Ponts ParisTech\xspace}
\newcommand{\api}{\gls{api}\xspace}
\newcommand{\eida}{\gls{eida}\xspace}
\newcommand{\vhs}{\gls{vhs}\xspace}
\newcommand{\dishas}{\gls{dishas}\xspace}
\newcommand{\ist}{\textsc{i}\ieme{}\xspace}
\newcommand{\ii}{\textsc{ii}\ieme{}\xspace}
\newcommand{\viii}{\textsc{viii}\ieme{}\xspace}
\newcommand{\ix}{\textsc{ix}\ieme{}\xspace}
\newcommand{\xie}{\textsc{xi}\ieme{}\xspace}
\newcommand{\xii}{\textsc{xii}\ieme{}\xspace}
\newcommand{\xiii}{\textsc{xiii}\ieme{}\xspace}
\newcommand{\xv}{\textsc{xv}\ieme{}\xspace}
\newcommand{\xvi}{\textsc{xvi}\ieme{}\xspace}
\newcommand{\xviii}{\textsc{xviii}\ieme{}\xspace}
\newcommand{\jc}{av. J.-C.\xspace}
\newcommand{\ma}{Moyen-Âge\xspace}
\newcommand{\ml}{\textit{machine learning}\xspace}
\newcommand{\dl}{\textit{deep learning}\xspace}
\newcommand{\cv}{\textit{computer vision}\xspace}
\newcommand{\ia}{intelligence artificielle\xspace}

\newcommand{\clearemptydoublepage}{\newpage{\pagestyle{empty}\cleardoublepage}}
% Pour des chapitres non numérotées dans la table des matière
\newcommand\chapterNo[1]{
  \chapter*{#1}
  \markright{\MakeUppercase{#1}}
}

\begin{document}

\onehalfspacing 

\frontmatter

    \include{templates/page-titre}

    \thispagestyle{empty}	
    \cleardoublepage
	
    \include{templates/resume}
	
    \chapterNo{Remerciements}
    \addcontentsline{toc}{chapter}{Remerciements}

    \printbibliography

    \chapterNo{Introduction}
    \addcontentsline{toc}{chapter}{Introduction}

    \thispagestyle{empty}
    \cleardoublepage

\mainmatter

    \part{Construire un corpus de numérisations pour le traitement par vision artificielle}
        \chapter{Le projet EiDA}
        Cette partie a pour objectif de revenir sur le contexte institutionnel du projet \eida, projet de recherche mené à l'Observatoire de Paris par l'équipe d'histoire des science du laboratoire \acrshort{syrte}. Ce projet a pour sujet d'étude les diagrammes astronomiques de tradition ptoléméenne : cette partie vise ainsi à contextualiser d'un point de vue historique le corpus du projet, et à en expliciter les bornes chronologiques et géographiques.
        
                \section{Contexte et objectifs du projet}
                    \input{templates/partie1/chap1/section1}

                \section{Sources primaires}
                    \input{templates/partie1/chap1/section2}
		\\
		
		\eida est un projet aux bornes chronologiques, géographiques et thématiques vastes, pour permettre une étude sur un temps long et dans un contexte global de la circulation des diagrammes astronomiques et des théories scientifiques qui les accompagnent. Dans cette démarche, proposer une étude quantitative, traitant un grand nombre de sources, permet de mettre en avant des motifs, connexions et évolutions en accord avec la diffusion afro-eurasienne des idées développées par Ptolémée. Les bornes définies dans cette partie permettent de délimiter un corpus d'images -- numérisations d'ouvrages manuscrits ou imprimés -- qui en tant d'objets numériques présentent également leurs propres problématiques, sur lesquelles revient la partie suivante.
        \clearemptydoublepage
        
        \chapter{Images et interopérabilité}
        Les sources étudiées dans ce mémoire sont des sources iconographiques : cette partie vise à revenir sur les formats, ressources et méthodes pour le traitement des images en ligne, de la production de la ressource à sa publication. Les images digitales présentent des enjeux spécifiques, du point de vue de la technique, du droit et de la disponibilité. La mise en ligne et la diffusion d'une image fait suite à une longue chaîne de traitement qui a pour point de départ un objet matériel, et soulève des questionnements divers. Les spécifications \iiif tente de répondre à un certain nombre de problématiques liées à la présence en ligne de ressources iconographiques, et cette partie vise donc à présenter les solutions, possibilités et limites offertes par ce standard.
        
                \section{L’image comme source}
                    \input{templates/partie1/chap2/section1}
            
                \section{Le standard IIIF}
                    \input{templates/partie1/chap2/section2}
        \\
        
        Les sources iconographiques sont soumises à un ensemble de restrictions, du point de vue du format, des métadonnées ou des droits, qui manquent encore d'une uniformité internationale et entre institutions qui rendrait fluide le partage de ces ressources sur Internet et entre les projets de recherche. Dans un projet impliquant l'utilisation d'algorithmes de vision artificielle, qui repose alors sur le traitement d'un volume important d'image, la mise en ligne de ces documents est un enjeux crucial, sur lequel repose d'une part la possibilité de constituer un corpus exploitable, ainsi que la publication des résultats du projet, qui peuvent également prendre la forme d'images numériques. Dans une optique de science ouverte, l'utilisation de standards et d'outils tels que ceux développés par le consortium \iiif permet d'assurer le partage d'images et de données respectant les mêmes formats, exploitables avec des outils libres, et d'avancer vers une abolition des silos de données, qui faciliterait notamment la construction de corpus massifs pour l'apprentissage machine. 
        \clearemptydoublepage
        
        \chapter[Corpus historiques et jeux de données]{Corpus historiques et jeux de données pour l’apprentissage machine}
        
        La vision artificielle et l'apprentissage machine permet le traitement de corpus d'images massifs par des méthodes quantitatives qui permettent aux historiens de traiter un volume de données bien plus important qu'une approche manuelle, ouvrant ainsi la voie à de nouvelles approches. Cette partie revient sur les bonnes pratiques à mettre en place afin d'assurer la pertinence de l'utilisation de ces outils, et d'en faire des traitements efficaces en accord avec les ambitions des projets.
        
                \section{Dimensions et cadre}
                    \input{templates/partie1/chap3/section1}
            
                \section{Objectifs scientifiques et possibilités numériques}
                    \input{templates/partie1/chap3/section2}
        \\
        
        Le \dl et la vision artificielle permettent aux projets de recherche en histoire et en histoire de l'art d'envisager de nouvelles approches des sources, à l'aide de traitement automatisés qui offrent de nouvelles méthodes de navigation de corpus d'images massifs : de la détection à l'édition, les outils produits redéfinissent les étapes de traitement des sources, et il est ainsi nécessaire d'intégrer aux pratiques des chercheurs des méthodes spécifiques à ce type d'approche, et notamment pour la création de jeu de données d'entraînement, à la base du développement de tout modèle de \ml. Cette intégration passe notamment par la rédaction de documentation, ainsi qu'un dialogue entre les équipes de recherche en vision artificielle et les équipes d'historiens. Ce dialogue permet d'établir les besoins de chacun, et de développer des outils techniques qui répondent aux besoins scientifiques de manière pertinente, en développant des modèles qui, malgré leurs limites, rejoignent aux mieux les attentes des sciences historiques.
        \clearemptydoublepage


    \part{De l’image à l’objet : intégrer l’apprentissage profond au traitement des sources historiques}
        \chapter[L'apprentissage profond]{Principes et utilisation de l’apprentissage profond}
        
        L'apprentissage profond est un sous-domaine de l'apprentissage automatique basé sur l'apprentissage de couches successives de représentation. Le nombre de couches définit la profondeur du modèle : de nos jours, l'apprentissage profond compte plusieurs dizaines à plusieurs centaines de couches, qui apprennent toutes automatiquement à l'aide de données d'apprentissage\footcite{cholletApprentissageProfondAvec2020a}. Cette approche est au cœur des modèles de vision artificielle dont nous parlons dans ce mémoire, qui reposent sur des réseaux de neurones qui constituent ces couches superposées permettant un apprentissage des représentations à partir de données fournies.
        
                \section{Réseaux de neurones et \textit{computer vision}}
                    \input{templates/partie2/chap4/section1}
            
                \section[Modèles de vision \textit{off-the-shelf}]{Modèles de détection \textit{off-the-shelf} : outils libres pour l'extraction d’objets}
                    \input{templates/partie2/chap4/section2}
        \\
		
		Les modèles de vision \textit{off-the-shelf} ouvrent à des projets divers la possibilité d'intégrer la vision artificielle à leurs méthodologies, en réduisant le coût humain et temporel du développement d'un modèle de \textit{deep learning} par la mise à disposition en accès libre d'outils déjà performants, qu'il est possible d'entraîner pour les ajuster à des données spécifiques. Pour la détection d'objets, des jeux de données en accès libre tels qu'ImageNet permettent un pré-entraînement de ces modèles \textit{off-the-shelf}, qui apprennent alors des caractéristiques larges qui peuvent être précisées par un entraînement sur des sources plus spécifiques, en nécessitant un volume de données moins important qu'un modèle créé de zéro : ces modèles de détection \textit{off-the-shelf} présentent ainsi une solution aux limites que peuvent présenter les sources historiques en termes de volume des données disponibles, et permettent également aux projets de se construire sur des bases solides, sans allouer de ressources à la création d'outils déjà existants, déjà performants, pour des tâches telles que la détection d'objet qui font partie des tâches canoniques de la vision par ordinateur.
        \clearemptydoublepage
        
        \chapter[Construire une plateforme pour la détection]{Construire une plateforme pour la détection : outils, interfaces et modèles de données}
        
        L'intégration du \textit{deep learning} aux pratiques des chercheurs en sciences historiques passe par le développement d'une plateforme qui leur permet d'exploiter simplement ces outils pour traiter leurs sources. Cette plateforme à interface graphique doit être adaptée aux sources traitées, et pensée pour intégrer toutes les fonctionnalités souhaitées. En s'appuyant sur les développements réalisés par les projets \eida et \vhs, cette partie revient sur la construction d'une application  et d'une \api dédiées à la détection de diagrammes et images scientifiques dans les numérisations d'ouvrages manuscrits ou imprimés, avec une interface pour la correction de la détection : nous évoquons ainsi les méthodes de développement, les réflexions liées aux données, les besoins matériels liés à l'utilisation d'algorithmes de vision, et le développement d'une \api pour lancer l'inférence sur un \gpu. 
         
                \section{Penser une application pour la détection d'objets}
                    \input{templates/partie2/chap5/section1}
            
                \section{Annoter sur un GPU : extractorAPI}
                    \input{templates/partie2/chap5/section2}
        \\
        
        L'\textit{open source} est une préoccupation centrale lors du développement d'outil pour les projets de recherche : il est en effet souhaitable de produire, dans la mesure du possible, des applications réutilisables par des projets futurs, pour assurer une continuité dans les travaux produits sans que se démultiplient les développements d'outils similaires. Ainsi, le remploi du code est au cœur des ambitions des équipes d'ingénierie : l'intégration de l'apprentissage profond à des projets d'histoire nécessite des outils spécifiques, construits autour de ces traitements des sources par l'\ia, qui restent néanmoins similaires -- ou comparables dans leurs besoins techniques -- d'un projet à l'autre. L'utilisation d'un \gpu pour la vision artificielle compte parmi les besoins matériels qui se traduisent entre les projets, ainsi, dans une optique de programmation modulaire, une \api externe à l'application \vhs/\eida a été développée pour répondre à ce besoin, sans être hautement spécifique aux besoins de ces projets comme pourrait l'être un module intégré à l'application. \exapi, l'\api développée dans ce cadre, est ainsi conçue, dès son cahier des charges, pour répondre aux besoins du projet \eida tout en restant réemployable dans d'autres contextes -- sont ainsi utilisés, pour sa construction, des outils libres qui en standardisent le développement, avec la vocation de produire une \api aussi flexible que robuste.
        
      	\chapter[\textit{Computer vision} et pratiques des chercheurs]{Intégrer la vision artificielle aux pratiques des chercheurs}
      	
      	Le chapitre précédent établit, du point de vue du \textit{back end}, les besoins techniques pour l'intégration de la vision artificielle à une chaîne de traitement des sources historiques : ces préoccupations en termes d'architecture et de puissance de calcul sont celles des équipes d'ingénierie, et existent en parallèle des préoccupations qui concernent plus directement les utilisateurs, et impactent réellement les pratiques des chercheurs. Ce chapitre revient ainsi sur la chaîne de traitement des sources historiques dans sa totalité, depuis la numérisation fournie par l'utilisateur jusqu'aux résultats retournés pour le traitement par les chercheurs, et sur les échanges avec les utilisateurs, en termes d'interface mais également en termes de médiation.
               
                \section[\textit{Workflow} de traitement des sources]{De la numérisation à l’annotation : l'automatisation du traitement des sources}
                    \input{templates/partie2/chap6/section1}
             
                \section{Médiation et documentation}
                    \input{templates/partie2/chap6/section2}
            
        \clearemptydoublepage

    \part{Perspectives pour le traitement des sources : vers un outil pour l’édition et la recherche}
        \chapter[Éditer des diagrammes]{Éditer des diagrammes : vectorisation et édition critique}
                \section{Édition critique des diagrammes astronomiques}
                    \input{templates/partie3/chap7/section1}
            
                \section[De l’image aux vecteurs]{De l’image aux vecteurs : la vision artificielle pour l’édition numérique}
                    \input{templates/partie3/chap7/section2}
            
        \clearemptydoublepage
        
        \chapter{Regroupement par similarité et \textit{clustering}}
                \section{\textit{Similarity retrieval} et navigation des corpus}
                    \input{templates/partie3/chap8/section1}
            
                \section{Le \textit{clustering} comme outil pour les chercheurs}
                    \input{templates/partie3/chap8/section2}
            
        \clearemptydoublepage
    
    \chapterNo{Conclusion}
    Étudier et exploiter les résultats automatiques : limites et perspectives pour les sciences historiques
    \addcontentsline{toc}{chapter}{Conclusion}

\appendix
    \part*{Annexes}	
    \addcontentsline{toc}{part}{Annexes}
    
    \chapter[Prepare custom data for training]{\label{YOLOv5Training}Prepare custom data for training using the YOLOv5 workflow}
	    \input{templates/annexes/yolotraining}
    \chapter[Modèles de données EIDA/VHS]{\label{eidaDataModels}Modèles de données des applications VHS et EIDA}
	    \input{templates/annexes/datamodel}
    \chapter{\label{exapiCahier}extractorAPI : cahier des charges}
	    \input{templates/annexes/exapicahier}
	\chapter{\label{yoloScript}Script YOLOv5 pour lancer la détection d'objet}
		Le script suivant est une reproduction du script \texttt{detect\_vhs.py} utilisé dans \exapi pour lancer la détection d'objet dans les images envoyées par l'application : \url{https://github.com/jnorindr/extractorAPI/blob/main/yolov5/detect_vhs.py}
		\input{templates/annexes/yoloscript}
	\chapter[Annotate images with extractorAPI]{\label{exapiAnno}Annotate images from EiDA with extractorAPI}
		Ce document est extrait du Wiki de l'\api, disponible sur GitHub en accompagnement du code. Il a été rédigé pour documenter les interactions entre \exapi et l'application \eida, pour faciliter la réutilisation de l'\api par des projets tiers. Le document est disponible à l'adresse suivante : \url{https://github.com/jnorindr/extractorAPI/wiki/Annotate-images-from-EiDA-with-extractorAPI}
		\input{templates/annexes/exapianno}

\clearemptydoublepage

\backmatter
    \printacronyms[title=Liste des acronymes,toctitle=Acronymes]
    \printglossary 
    \listoffigures
    \tableofcontents

	
\end{document}